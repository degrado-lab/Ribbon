{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ribbon Ribbon is a python package which simplifies the usage and pipelining of biological software. Installing and running state-of-the-art tools can be done with a simple python script. Quick Start Ready to dive in? Follow these simple steps: Installation: Get started by following our Installation Guide which details how to set up Ribbon on your system. Getting Started: Explore practical examples in our Usage Guide that demonstrate common workflows. Source Code Please visit our GitHub repo for the Ribbon source code. Ribbon is open-source, and MIT licensed.","title":"Home"},{"location":"#ribbon","text":"Ribbon is a python package which simplifies the usage and pipelining of biological software. Installing and running state-of-the-art tools can be done with a simple python script.","title":"Ribbon"},{"location":"#quick-start","text":"Ready to dive in? Follow these simple steps: Installation: Get started by following our Installation Guide which details how to set up Ribbon on your system. Getting Started: Explore practical examples in our Usage Guide that demonstrate common workflows.","title":"Quick Start"},{"location":"#source-code","text":"Please visit our GitHub repo for the Ribbon source code. Ribbon is open-source, and MIT licensed.","title":"Source Code"},{"location":"contribute/","text":"[!WARNING] \ud83d\udea7 This Page is Under Construction! \ud83d\udea7","title":"Contributing"},{"location":"cookbook/","text":"Cookbook Chai-1 Chai-1 is a high accuracy, ligand-aware protein folding tool. It takes as input a FASTA file, and outputs a directory of folded PDB structures. Chai-1 scores are output as .npz files. Simple Fold a protein from a sequence ribbon.Chai1( fasta_file = 'my_sequence.fasta', # A single input FASTA. If there are multiple sequences, they will be folded in the same structure. output_dir = './out' # Where the outputs will be stored ).run() Advanced Fold a protein from a sequence. Include two copies of ligand ribbon.Chai1( fasta_file = 'my_sequence.fasta', # A single input FASTA. If there are multiple sequences, they will be folded in the same structure. output_dir = './out', # Where the outputs will be stored smiles_string = 'C1=CC=CC=C1', # SMILES string of our ligand num_ligands = 2, # How many copies of our ligand? device = 'gpu' # Run on GPU (necessary for Chai-1) ).run() LigandMPNN LigandMPNN is a ligand-aware tool for designing sequences for a backbone structure. It takes as input a list of PDB files, and outputs a sequence (or sequences) that are predicted to fold into that structure. While LigandMPNN is gpu-accelerated, it seems to run fast on the CPU as well. Simple Design a sequence for a backbone: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], # List of PDB files output_dir = './out' # Output directory num_designs = 5 # How many sequences should we generate? ).run() The output folder will have the following structure: output_dir/ \u251c\u2500 backbones/ # Backbone structures with labeled AAs (but no sidechains) \u251c\u2500 packed/ # (Optional) Backbone structures with packed sidechains \u251c\u2500 seqs/ # A single FASTA containing the reference sequence and all designed sequences \u2514\u2500 seqs_split/ # Individual FASTA files for each designed sequence Advanced Design a homodimeric sequence, keeping crucial residues fixed. This example uses the extra_args parameter to add extra parameters into your run command. Note that this can inject arbitrary code into your container - use with caution! ligandmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], # List of PDB files output_dir = './out' # Output directory num_designs = 5 # How many sequences should we generate? extra_args= '--fixed_residues \\\"' + RESIDUES_TO_KEEP + '\\\" --homo_oligomer 1' # Make sure to keep my catalytric residues, and make two chains identical. ) RaptorX-Single RaptorX-Single is a fast protein folding tool. It can fold small structures in as little as 5 seconds, after an initial loading period. It takes as input a FASTA file (or directory containing FASTAs), and outputs a directory of folded PDB structures. Simple Fold a directory of FASTA files ribbon.RaptorXSingle( fasta_file_or_dir = './my_FASTA_directory/', output_dir = './out' ).run() Advanced Fold a directory of FASTA files using a non-default model checkpoint (param). Run on the CPU (slower; GPU is default). ribbon.RaptorXSingle( fasta_file_or_dir = './my_FASTA_directory/', output_dir = './out', param = 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt', device='cpu' ).run() The available model parameters are: 'RaptorX-Single-ESM1b.pt', 'RaptorX-Single-ESM1v.pt', 'RaptorX-Single-ProtTrans.pt', 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt', 'RaptorX-Single-ESM1b-Ab.pt', 'RaptorX-Single-ESM1v-Ab.pt', 'RaptorX-Single-ProtTrans-Ab.pt', 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt'","title":"Cookbook"},{"location":"cookbook/#cookbook","text":"","title":"Cookbook"},{"location":"cookbook/#chai-1","text":"Chai-1 is a high accuracy, ligand-aware protein folding tool. It takes as input a FASTA file, and outputs a directory of folded PDB structures. Chai-1 scores are output as .npz files.","title":"Chai-1"},{"location":"cookbook/#simple","text":"Fold a protein from a sequence ribbon.Chai1( fasta_file = 'my_sequence.fasta', # A single input FASTA. If there are multiple sequences, they will be folded in the same structure. output_dir = './out' # Where the outputs will be stored ).run()","title":"Simple"},{"location":"cookbook/#advanced","text":"Fold a protein from a sequence. Include two copies of ligand ribbon.Chai1( fasta_file = 'my_sequence.fasta', # A single input FASTA. If there are multiple sequences, they will be folded in the same structure. output_dir = './out', # Where the outputs will be stored smiles_string = 'C1=CC=CC=C1', # SMILES string of our ligand num_ligands = 2, # How many copies of our ligand? device = 'gpu' # Run on GPU (necessary for Chai-1) ).run()","title":"Advanced"},{"location":"cookbook/#ligandmpnn","text":"LigandMPNN is a ligand-aware tool for designing sequences for a backbone structure. It takes as input a list of PDB files, and outputs a sequence (or sequences) that are predicted to fold into that structure. While LigandMPNN is gpu-accelerated, it seems to run fast on the CPU as well.","title":"LigandMPNN"},{"location":"cookbook/#simple_1","text":"Design a sequence for a backbone: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], # List of PDB files output_dir = './out' # Output directory num_designs = 5 # How many sequences should we generate? ).run() The output folder will have the following structure: output_dir/ \u251c\u2500 backbones/ # Backbone structures with labeled AAs (but no sidechains) \u251c\u2500 packed/ # (Optional) Backbone structures with packed sidechains \u251c\u2500 seqs/ # A single FASTA containing the reference sequence and all designed sequences \u2514\u2500 seqs_split/ # Individual FASTA files for each designed sequence","title":"Simple"},{"location":"cookbook/#advanced_1","text":"Design a homodimeric sequence, keeping crucial residues fixed. This example uses the extra_args parameter to add extra parameters into your run command. Note that this can inject arbitrary code into your container - use with caution! ligandmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], # List of PDB files output_dir = './out' # Output directory num_designs = 5 # How many sequences should we generate? extra_args= '--fixed_residues \\\"' + RESIDUES_TO_KEEP + '\\\" --homo_oligomer 1' # Make sure to keep my catalytric residues, and make two chains identical. )","title":"Advanced"},{"location":"cookbook/#raptorx-single","text":"RaptorX-Single is a fast protein folding tool. It can fold small structures in as little as 5 seconds, after an initial loading period. It takes as input a FASTA file (or directory containing FASTAs), and outputs a directory of folded PDB structures.","title":"RaptorX-Single"},{"location":"cookbook/#simple_2","text":"Fold a directory of FASTA files ribbon.RaptorXSingle( fasta_file_or_dir = './my_FASTA_directory/', output_dir = './out' ).run()","title":"Simple"},{"location":"cookbook/#advanced_2","text":"Fold a directory of FASTA files using a non-default model checkpoint (param). Run on the CPU (slower; GPU is default). ribbon.RaptorXSingle( fasta_file_or_dir = './my_FASTA_directory/', output_dir = './out', param = 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt', device='cpu' ).run() The available model parameters are: 'RaptorX-Single-ESM1b.pt', 'RaptorX-Single-ESM1v.pt', 'RaptorX-Single-ProtTrans.pt', 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt', 'RaptorX-Single-ESM1b-Ab.pt', 'RaptorX-Single-ESM1v-Ab.pt', 'RaptorX-Single-ProtTrans-Ab.pt', 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt'","title":"Advanced"},{"location":"design/","text":"[!WARNING] \ud83d\udea7 This Page is Under Construction! \ud83d\udea7","title":"Design Notes"},{"location":"installation/","text":"Apptainer is the only requirement to run Ribbon. Install apptainer for your system here . We recommend creating a fresh environment for Ribbon: conda create --name ribbon python=3.12 -y conda activate ribbon Then, install Ribbon in a clean python environment: pip install ribbon-toolkit","title":"Installation"},{"location":"api/config/","text":"","title":"Config"},{"location":"api/deserialize_and_run/","text":"","title":"Serialization"},{"location":"api/runner/","text":"Task Source code in ribbon/runner.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Task : def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" ) def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \" __init__ ( device = 'cpu' , extra_args = '' ) The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Parameters: device ( str , default: 'cpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task Returns: \u2013 None Source code in ribbon/runner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None __repr__ () Returns a string representation of the Task object. Source code in ribbon/runner.py 220 221 222 223 224 225 def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \" _get_task_dict ( task_name ) Returns the dictionary for a given task. Source code in ribbon/runner.py 197 198 199 200 201 202 203 204 205 def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] _get_task_inputs ( task_name ) Returns the inputs required for a given task Source code in ribbon/runner.py 207 208 209 210 211 212 213 214 215 216 217 218 def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs _run_task ( task_name , scheduler = 'local' , device = 'gpu' , extra_args = '' , container_override = None , ** kwargs ) Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Parameters: task_name ( str ) \u2013 The name of the task to run. device ( str , default: 'gpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override ( str , default: None ) \u2013 The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs ( dict , default: {} ) \u2013 Task-specific keyword arguments. Returns: \u2013 None Source code in ribbon/runner.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) queue ( scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ) Queue the LigandMPNN task using the given scheduler. Parameters: scheduler ( str ) \u2013 The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on ( list , default: [] ) \u2013 A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks ( int , default: 1 ) \u2013 The number of tasks to run. Defaults to 1. time ( str , default: '1:00:00' ) \u2013 The time to allocate for the task. Defaults to '1:00:00'. mem ( str , default: '2G' ) \u2013 The memory to allocate for the task. Defaults to '2G'. auto_restart ( bool , default: True ) \u2013 Whether to automatically restart the task if it fails. Defaults to True. other_resources ( dict , default: {} ) \u2013 Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name ( str , default: None ) \u2013 The name of the job. Defaults to None. output_file ( str , default: None ) \u2013 The file to write the output to. Defaults to None. queue ( str , default: None ) \u2013 The queue to submit the task to. Defaults to None. gpus ( int , default: None ) \u2013 The number of GPUs to allocate for the task. Defaults to None. node_name ( str , default: None ) \u2013 The name of the node to run the task on. Defaults to None. Returns: str \u2013 The ID of the job in the scheduler. Source code in ribbon/runner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id run () Run the task. This method should be overridden by the child class. Source code in ribbon/runner.py 25 26 27 28 29 def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" )","title":"Core Ribbon Task"},{"location":"api/runner/#ribbon.runner.Task","text":"Source code in ribbon/runner.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Task : def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" ) def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \"","title":"Task"},{"location":"api/runner/#ribbon.runner.Task.__init__","text":"The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Parameters: device ( str , default: 'cpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task Returns: \u2013 None Source code in ribbon/runner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None","title":"__init__"},{"location":"api/runner/#ribbon.runner.Task.__repr__","text":"Returns a string representation of the Task object. Source code in ribbon/runner.py 220 221 222 223 224 225 def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \"","title":"__repr__"},{"location":"api/runner/#ribbon.runner.Task._get_task_dict","text":"Returns the dictionary for a given task. Source code in ribbon/runner.py 197 198 199 200 201 202 203 204 205 def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ]","title":"_get_task_dict"},{"location":"api/runner/#ribbon.runner.Task._get_task_inputs","text":"Returns the inputs required for a given task Source code in ribbon/runner.py 207 208 209 210 211 212 213 214 215 216 217 218 def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs","title":"_get_task_inputs"},{"location":"api/runner/#ribbon.runner.Task._run_task","text":"Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Parameters: task_name ( str ) \u2013 The name of the task to run. device ( str , default: 'gpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override ( str , default: None ) \u2013 The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs ( dict , default: {} ) \u2013 Task-specific keyword arguments. Returns: \u2013 None Source code in ribbon/runner.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' )","title":"_run_task"},{"location":"api/runner/#ribbon.runner.Task.queue","text":"Queue the LigandMPNN task using the given scheduler. Parameters: scheduler ( str ) \u2013 The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on ( list , default: [] ) \u2013 A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks ( int , default: 1 ) \u2013 The number of tasks to run. Defaults to 1. time ( str , default: '1:00:00' ) \u2013 The time to allocate for the task. Defaults to '1:00:00'. mem ( str , default: '2G' ) \u2013 The memory to allocate for the task. Defaults to '2G'. auto_restart ( bool , default: True ) \u2013 Whether to automatically restart the task if it fails. Defaults to True. other_resources ( dict , default: {} ) \u2013 Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name ( str , default: None ) \u2013 The name of the job. Defaults to None. output_file ( str , default: None ) \u2013 The file to write the output to. Defaults to None. queue ( str , default: None ) \u2013 The queue to submit the task to. Defaults to None. gpus ( int , default: None ) \u2013 The number of GPUs to allocate for the task. Defaults to None. node_name ( str , default: None ) \u2013 The name of the node to run the task on. Defaults to None. Returns: str \u2013 The ID of the job in the scheduler. Source code in ribbon/runner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id","title":"queue"},{"location":"api/runner/#ribbon.runner.Task.run","text":"Run the task. This method should be overridden by the child class. Source code in ribbon/runner.py 25 26 27 28 29 def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" )","title":"run"},{"location":"api/utils/","text":"clean_cache ( all = False ) Cleans the cache directory. If all=True, deletes all files. Otherwise, deletes only files that are older than 1 day. Source code in ribbon/utils.py 162 163 164 165 166 167 168 169 170 def clean_cache ( all = False ): \"\"\" Cleans the cache directory. If all=True, deletes all files. Otherwise, deletes only files that are older than 1 day. \"\"\" for file in os . listdir ( TASK_CACHE_DIR ): file = Path ( file ) if all or ( datetime . datetime . now () - datetime . datetime . fromtimestamp ( file . stat () . st_mtime )) . days > 1 : os . remove ( file ) deserialize ( filename , cache_dir = None ) Loads a Python object from a file. Parameters: filename \u2013 the filename to load the object from. cache_dir \u2013 the directory to load the object from. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: object \u2013 the Python object loaded from the file. Source code in ribbon/utils.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def deserialize ( filename , cache_dir = None ): \"\"\"Loads a Python object from a file. Args: filename: the filename to load the object from. cache_dir: the directory to load the object from. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: object: the Python object loaded from the file. \"\"\" # Make sure we have the full path: if cache_dir is None : cache_dir = TASK_CACHE_DIR cache_dir = make_directory ( cache_dir ) filename = Path ( filename ) if not filename . is_absolute (): filename = cache_dir / filename with open ( filename , 'rb' ) as f : return pickle . load ( f ) download_container ( container_local_path , container_ORAS_URL ) Downloads a container to the download directory, DOWNLOAD_DIR, from ribbon.config. Parameters: container_local_path ( str ) \u2013 The path to the container to download. container_ORAS_URL ( str ) \u2013 The ORAS URL of the container to download. Returns: \u2013 None Source code in ribbon/utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def download_container ( container_local_path , container_ORAS_URL ): \"\"\" Downloads a container to the download directory, DOWNLOAD_DIR, from ribbon.config. Args: container_local_path (str): The path to the container to download. container_ORAS_URL (str): The ORAS URL of the container to download. Returns: None \"\"\" # Make sure downloads directory exists: make_directories ( DOWNLOAD_DIR ) # Download the container to the download_dir command = f 'apptainer pull { container_local_path } { container_ORAS_URL } ' run_command ( command ) return # Get error codes, etc. list_files ( directory , extension ) Returns a list of files in a directory with a given extension Source code in ribbon/utils.py 15 16 17 def list_files ( directory , extension ): \"\"\"Returns a list of files in a directory with a given extension\"\"\" return [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if f . endswith ( extension )] make_directories ( * directories ) Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. Source code in ribbon/utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_directories ( * directories ): \"\"\" Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. \"\"\" new_directories = [] for directory in directories : # Check it's a Path object: if not isinstance ( directory , Path ): directory = Path ( directory ) directory . mkdir ( parents = True , exist_ok = True ) new_directories . append ( directory ) return new_directories make_directory ( directory ) Creates a directory if it does not exist. Parameters: directory ( str or Path ) \u2013 The directory to create. Returns: Path \u2013 path object of the created directory. Source code in ribbon/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def make_directory ( directory ): \"\"\" Creates a directory if it does not exist. Args: directory (str or Path): The directory to create. Returns: Path: path object of the created directory. \"\"\" directory = make_directories ( directory )[ 0 ] return directory run_command ( command , capture_output = False ) Runs a command in the shell. If capture_output=True, returns the stdout and stderr. Otherwise, returns prints the stdout and stderr. Source code in ribbon/utils.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def run_command ( command , capture_output = False ): \"\"\" Runs a command in the shell. If capture_output=True, returns the stdout and stderr. Otherwise, returns prints the stdout and stderr. \"\"\" # Run the container stdout , stderr = None , None print ( 'Running command:' , command ) if capture_output : process = subprocess . run ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) # return stdout and stderr stdout , stderr = process . stdout . decode ( 'utf-8' ), process . stderr . decode ( 'utf-8' ) else : process = subprocess . run ( command , shell = True ) return stdout , stderr serialize ( obj , save_dir = None ) Saves a Python object to a file. A random filename is generated, and it is saved to the save_dir. Parameters: obj \u2013 the Python object to save. save_dir \u2013 the directory to save the object. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: Path \u2013 path object of the saved file. Source code in ribbon/utils.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def serialize ( obj , save_dir = None ): \"\"\"Saves a Python object to a file. A random filename is generated, and it is saved to the save_dir. Args: obj: the Python object to save. save_dir: the directory to save the object. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: Path: path object of the saved file. \"\"\" if save_dir is None : save_dir = TASK_CACHE_DIR # Make sure the directory exists: print ( save_dir ) save_dir = make_directory ( save_dir ) print ( 'Saving object to:' , save_dir ) # Generate a random filename: filename = save_dir / f ' { uuid . uuid4 () } .pkl' # Save the object: with open ( filename , 'wb' ) as f : pickle . dump ( obj , f ) return filename verify_container ( software_name ) Verifies that the container for the given software is downloaded. If not, downloads it to DOWNLOAD_DIR from ribbon.config. Parameters: software_name ( str ) \u2013 The name of the software to verify the container for. Returns: str \u2013 The path to the downloaded container. Source code in ribbon/utils.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def verify_container ( software_name ): \"\"\" Verifies that the container for the given software is downloaded. If not, downloads it to DOWNLOAD_DIR from ribbon.config. Args: software_name (str): The name of the software to verify the container for. Returns: str: The path to the downloaded container. \"\"\" # Get the container local path and ORAS URL: import json print ( 'TASKS_DIR:' , TASKS_DIR ) with open ( TASKS_DIR / 'containers.json' ) as f : containers = json . load ( f ) # Our database maps software names to container names and ORAS URLs # Example: {\"LigandMPNN\": [\"ligandMPNN.sif\", \"oras://docker.io/nicholasfreitas/ligandmpnn:latest\"]} container_local_name , container_ORAS_URL = containers [ software_name ] container_local_path = DOWNLOAD_DIR / container_local_name # Is the container already downloaded? if not os . path . exists ( container_local_path ): # If not, download the container download_container ( container_local_path , container_ORAS_URL ) return container_local_path wait_for_jobs ( job_ids , scheduler , max_wait = 3600 ) Waits for a list of job IDs to complete. Returns when all jobs are completed, or when max_wait is exceeded. Parameters: job_ids ( list ) \u2013 list of job IDs scheduler ( str ) \u2013 the scheduler to use. SGE or SLURM. max_wait ( int , default: 3600 ) \u2013 maximum time to wait in seconds. Default is 1 hour. Returns: \u2013 None TODO: Add kill_after parameter to kill jobs if we exceed max_wait. Default false. Source code in ribbon/utils.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def wait_for_jobs ( job_ids , scheduler , max_wait = 3600 ): \"\"\" Waits for a list of job IDs to complete. Returns when all jobs are completed, or when max_wait is exceeded. Args: job_ids (list): list of job IDs scheduler (str): the scheduler to use. SGE or SLURM. max_wait (int): maximum time to wait in seconds. Default is 1 hour. Returns: None TODO: Add kill_after parameter to kill jobs if we exceed max_wait. Default false. \"\"\" start_time = datetime . datetime . now () if scheduler == 'SGE' : check_job_status = sge_check_job_status elif scheduler == 'SLURM' : check_job_status = slurm_check_job_status else : raise ValueError ( 'Invalid scheduler. Must be SGE or SLURM.' ) # Print status: waiting_for = len ( job_ids ) print ( f 'Waiting for { waiting_for } jobs to complete...' ) while True : # Check if all jobs are completed: all_completed = True not_finished_count = 0 statuses = check_job_status ( job_ids ) for job_id , status in statuses . items (): if status == 'not completed' : all_completed = False not_finished_count += 1 if all_completed : break # All jobs are completed, we're done! # Print status, only when it changes: if not_finished_count != waiting_for : waiting_for = not_finished_count print ( f 'Waiting for { waiting_for } jobs to complete...' ) # Check if we've waited too long: elapsed_time = ( datetime . datetime . now () - start_time ) . seconds if elapsed_time > max_wait : print ( 'Max wait time exceeded. Exiting.' ) break # Wait for a bit before checking again: time . sleep ( 10 ) return","title":"Utils"},{"location":"api/utils/#ribbon.utils.clean_cache","text":"Cleans the cache directory. If all=True, deletes all files. Otherwise, deletes only files that are older than 1 day. Source code in ribbon/utils.py 162 163 164 165 166 167 168 169 170 def clean_cache ( all = False ): \"\"\" Cleans the cache directory. If all=True, deletes all files. Otherwise, deletes only files that are older than 1 day. \"\"\" for file in os . listdir ( TASK_CACHE_DIR ): file = Path ( file ) if all or ( datetime . datetime . now () - datetime . datetime . fromtimestamp ( file . stat () . st_mtime )) . days > 1 : os . remove ( file )","title":"clean_cache"},{"location":"api/utils/#ribbon.utils.deserialize","text":"Loads a Python object from a file. Parameters: filename \u2013 the filename to load the object from. cache_dir \u2013 the directory to load the object from. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: object \u2013 the Python object loaded from the file. Source code in ribbon/utils.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def deserialize ( filename , cache_dir = None ): \"\"\"Loads a Python object from a file. Args: filename: the filename to load the object from. cache_dir: the directory to load the object from. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: object: the Python object loaded from the file. \"\"\" # Make sure we have the full path: if cache_dir is None : cache_dir = TASK_CACHE_DIR cache_dir = make_directory ( cache_dir ) filename = Path ( filename ) if not filename . is_absolute (): filename = cache_dir / filename with open ( filename , 'rb' ) as f : return pickle . load ( f )","title":"deserialize"},{"location":"api/utils/#ribbon.utils.download_container","text":"Downloads a container to the download directory, DOWNLOAD_DIR, from ribbon.config. Parameters: container_local_path ( str ) \u2013 The path to the container to download. container_ORAS_URL ( str ) \u2013 The ORAS URL of the container to download. Returns: \u2013 None Source code in ribbon/utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def download_container ( container_local_path , container_ORAS_URL ): \"\"\" Downloads a container to the download directory, DOWNLOAD_DIR, from ribbon.config. Args: container_local_path (str): The path to the container to download. container_ORAS_URL (str): The ORAS URL of the container to download. Returns: None \"\"\" # Make sure downloads directory exists: make_directories ( DOWNLOAD_DIR ) # Download the container to the download_dir command = f 'apptainer pull { container_local_path } { container_ORAS_URL } ' run_command ( command ) return # Get error codes, etc.","title":"download_container"},{"location":"api/utils/#ribbon.utils.list_files","text":"Returns a list of files in a directory with a given extension Source code in ribbon/utils.py 15 16 17 def list_files ( directory , extension ): \"\"\"Returns a list of files in a directory with a given extension\"\"\" return [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if f . endswith ( extension )]","title":"list_files"},{"location":"api/utils/#ribbon.utils.make_directories","text":"Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. Source code in ribbon/utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_directories ( * directories ): \"\"\" Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. \"\"\" new_directories = [] for directory in directories : # Check it's a Path object: if not isinstance ( directory , Path ): directory = Path ( directory ) directory . mkdir ( parents = True , exist_ok = True ) new_directories . append ( directory ) return new_directories","title":"make_directories"},{"location":"api/utils/#ribbon.utils.make_directory","text":"Creates a directory if it does not exist. Parameters: directory ( str or Path ) \u2013 The directory to create. Returns: Path \u2013 path object of the created directory. Source code in ribbon/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def make_directory ( directory ): \"\"\" Creates a directory if it does not exist. Args: directory (str or Path): The directory to create. Returns: Path: path object of the created directory. \"\"\" directory = make_directories ( directory )[ 0 ] return directory","title":"make_directory"},{"location":"api/utils/#ribbon.utils.run_command","text":"Runs a command in the shell. If capture_output=True, returns the stdout and stderr. Otherwise, returns prints the stdout and stderr. Source code in ribbon/utils.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def run_command ( command , capture_output = False ): \"\"\" Runs a command in the shell. If capture_output=True, returns the stdout and stderr. Otherwise, returns prints the stdout and stderr. \"\"\" # Run the container stdout , stderr = None , None print ( 'Running command:' , command ) if capture_output : process = subprocess . run ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) # return stdout and stderr stdout , stderr = process . stdout . decode ( 'utf-8' ), process . stderr . decode ( 'utf-8' ) else : process = subprocess . run ( command , shell = True ) return stdout , stderr","title":"run_command"},{"location":"api/utils/#ribbon.utils.serialize","text":"Saves a Python object to a file. A random filename is generated, and it is saved to the save_dir. Parameters: obj \u2013 the Python object to save. save_dir \u2013 the directory to save the object. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: Path \u2013 path object of the saved file. Source code in ribbon/utils.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def serialize ( obj , save_dir = None ): \"\"\"Saves a Python object to a file. A random filename is generated, and it is saved to the save_dir. Args: obj: the Python object to save. save_dir: the directory to save the object. If None, uses TASK_CACHE_DIR from ribbon.config. Returns: Path: path object of the saved file. \"\"\" if save_dir is None : save_dir = TASK_CACHE_DIR # Make sure the directory exists: print ( save_dir ) save_dir = make_directory ( save_dir ) print ( 'Saving object to:' , save_dir ) # Generate a random filename: filename = save_dir / f ' { uuid . uuid4 () } .pkl' # Save the object: with open ( filename , 'wb' ) as f : pickle . dump ( obj , f ) return filename","title":"serialize"},{"location":"api/utils/#ribbon.utils.verify_container","text":"Verifies that the container for the given software is downloaded. If not, downloads it to DOWNLOAD_DIR from ribbon.config. Parameters: software_name ( str ) \u2013 The name of the software to verify the container for. Returns: str \u2013 The path to the downloaded container. Source code in ribbon/utils.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def verify_container ( software_name ): \"\"\" Verifies that the container for the given software is downloaded. If not, downloads it to DOWNLOAD_DIR from ribbon.config. Args: software_name (str): The name of the software to verify the container for. Returns: str: The path to the downloaded container. \"\"\" # Get the container local path and ORAS URL: import json print ( 'TASKS_DIR:' , TASKS_DIR ) with open ( TASKS_DIR / 'containers.json' ) as f : containers = json . load ( f ) # Our database maps software names to container names and ORAS URLs # Example: {\"LigandMPNN\": [\"ligandMPNN.sif\", \"oras://docker.io/nicholasfreitas/ligandmpnn:latest\"]} container_local_name , container_ORAS_URL = containers [ software_name ] container_local_path = DOWNLOAD_DIR / container_local_name # Is the container already downloaded? if not os . path . exists ( container_local_path ): # If not, download the container download_container ( container_local_path , container_ORAS_URL ) return container_local_path","title":"verify_container"},{"location":"api/utils/#ribbon.utils.wait_for_jobs","text":"Waits for a list of job IDs to complete. Returns when all jobs are completed, or when max_wait is exceeded. Parameters: job_ids ( list ) \u2013 list of job IDs scheduler ( str ) \u2013 the scheduler to use. SGE or SLURM. max_wait ( int , default: 3600 ) \u2013 maximum time to wait in seconds. Default is 1 hour. Returns: \u2013 None TODO: Add kill_after parameter to kill jobs if we exceed max_wait. Default false. Source code in ribbon/utils.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def wait_for_jobs ( job_ids , scheduler , max_wait = 3600 ): \"\"\" Waits for a list of job IDs to complete. Returns when all jobs are completed, or when max_wait is exceeded. Args: job_ids (list): list of job IDs scheduler (str): the scheduler to use. SGE or SLURM. max_wait (int): maximum time to wait in seconds. Default is 1 hour. Returns: None TODO: Add kill_after parameter to kill jobs if we exceed max_wait. Default false. \"\"\" start_time = datetime . datetime . now () if scheduler == 'SGE' : check_job_status = sge_check_job_status elif scheduler == 'SLURM' : check_job_status = slurm_check_job_status else : raise ValueError ( 'Invalid scheduler. Must be SGE or SLURM.' ) # Print status: waiting_for = len ( job_ids ) print ( f 'Waiting for { waiting_for } jobs to complete...' ) while True : # Check if all jobs are completed: all_completed = True not_finished_count = 0 statuses = check_job_status ( job_ids ) for job_id , status in statuses . items (): if status == 'not completed' : all_completed = False not_finished_count += 1 if all_completed : break # All jobs are completed, we're done! # Print status, only when it changes: if not_finished_count != waiting_for : waiting_for = not_finished_count print ( f 'Waiting for { waiting_for } jobs to complete...' ) # Check if we've waited too long: elapsed_time = ( datetime . datetime . now () - start_time ) . seconds if elapsed_time > max_wait : print ( 'Max wait time exceeded. Exiting.' ) break # Wait for a bit before checking again: time . sleep ( 10 ) return","title":"wait_for_jobs"},{"location":"api/batch/batch/","text":"generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) Generate an SGE command to submit a job to the scheduler. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. other_resources ( dict ) \u2013 A dictionary of other resources to pass to the scheduler. job_variables ( str ) \u2013 A string of environment variables to pass to the job. scheduler_script ( str ) \u2013 The path to the script to run. Returns: str \u2013 The SGE command to submit Source code in ribbon/batch/queue_utils.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def generate_sge_command ( resources , other_resources , job_variables , scheduler_script ): \"\"\" Generate an SGE command to submit a job to the scheduler. Args: resources (dict): A dictionary of resources to request for the job. other_resources (dict): A dictionary of other resources to pass to the scheduler. job_variables (str): A string of environment variables to pass to the job. scheduler_script (str): The path to the script to run. Returns: str: The SGE command to submit \"\"\" scheduler_command = 'qsub' # Map resources to SGE options resources_string = parse_sge_resources ( resources ) # Add other resources as-is, from dict: for key , value in other_resources . items (): if value == '' : # If value is empty, assume it's a flag without a value resources_string += f \" { key } \" else : if key . startswith ( '-l' ): resources_string += f \" { key } = { value } \" else : resources_string += f \" { key } { value } \" # Construct the command command = f \" { scheduler_command } -v { job_variables } { resources_string } { scheduler_script } \" return command generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) Generate a SLURM command to submit a job to the scheduler. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. other_resources ( dict ) \u2013 A dictionary of other resources to pass to the scheduler. job_variables ( str ) \u2013 A string of environment variables to pass to the job. scheduler_script ( str ) \u2013 The path to the script to run. Returns: str \u2013 The SLURM command to submit Source code in ribbon/batch/queue_utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ): \"\"\" Generate a SLURM command to submit a job to the scheduler. Args: resources (dict): A dictionary of resources to request for the job. other_resources (dict): A dictionary of other resources to pass to the scheduler. job_variables (str): A string of environment variables to pass to the job. scheduler_script (str): The path to the script to run. Returns: str: The SLURM command to submit \"\"\" scheduler_command = 'sbatch' # Map resources to SLURM options resources_string = parse_slurm_resources ( resources ) # Add other resources as-is, from dict: for key , value in other_resources . items (): if value == '' : # If value is empty, assume it's a flag without a value resources_string += f \" { key } \" else : resources_string += f \" { key } = { value } \" # Construct the command command = f \" { scheduler_command } --export= { job_variables } { resources_string } { scheduler_script } \" return command parse_sge_resources ( resources , dependency_type = None ) Parse a dictionary of resources into a string of SGE options. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. Returns: str \u2013 A string of SGE options TODO: implement dependency handling Source code in ribbon/batch/queue_utils.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def parse_sge_resources ( resources , dependency_type = None ): \"\"\" Parse a dictionary of resources into a string of SGE options. Args: resources (dict): A dictionary of resources to request for the job. Returns: str: A string of SGE options TODO: implement dependency handling \"\"\" resource_mappings = { 'time' : '-l h_rt' , 'mem' : '-l mem_free' , 'dependency' : '-hold_jid' , 'gpus' : '-l gpu' , 'job-name' : '-N' , 'output' : '-o' , 'queue' : '-q' , 'node-name' : '-l hostname' , # Add other resource mappings as needed } # Parse dependencies: if 'dependency' in resources : dependencies = resources [ 'dependency' ] if isinstance ( dependencies , list ): dependencies = ',' . join ([ str ( job_id ) for job_id in dependencies ]) resources [ 'dependency' ] = dependencies resources_list = [] for key , value in resources . items (): if key == 'dependency' : # Handle dependencies specifically resources_list . append ( f \"-hold_jid { value } \" ) else : if key not in resource_mappings : print ( f \"Warning: Unrecognized resource key: { key } . Skipping.\" ) continue sge_option = resource_mappings . get ( key ) if sge_option : if sge_option . startswith ( '-l' ): resources_list . append ( f \" { sge_option } = { value } \" ) else : resources_list . append ( f \" { sge_option } { value } \" ) else : # For unrecognized keys, assume they are '-l key=value' resources_list . append ( f \"-l { key } = { value } \" ) resources_string = ' ' . join ( resources_list ) return resources_string parse_slurm_resources ( resources , dependency_type = 'afterok' ) Parse a dictionary of resources into a string of SLURM options. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency to use (e.g. 'afterok', 'afterany', 'afternotok') Returns: str \u2013 A string of SLURM options Source code in ribbon/batch/queue_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def parse_slurm_resources ( resources , dependency_type = 'afterok' ): \"\"\" Parse a dictionary of resources into a string of SLURM options. Args: resources (dict): A dictionary of resources to request for the job. dependency_type (str): The type of dependency to use (e.g. 'afterok', 'afterany', 'afternotok') Returns: str: A string of SLURM options \"\"\" resource_mappings = { 'time' : '--time' , 'mem' : '--mem' , 'dependency' : '--dependency' , 'gpus' : '--gpus' , 'job-name' : '--job-name' , 'requeue' : '--requeue' , 'output' : '--output' , 'queue' : '--partition' , 'node-name' : '--nodelist' , # Add other resource mappings as needed } # Parse dependencies: if 'dependency' in resources : dependencies = resources [ 'dependency' ] if isinstance ( dependencies , list ): dependencies = ':' . join ([ str ( job_id ) for job_id in dependencies ]) resources [ 'dependency' ] = dependency_type + ':' + dependencies resources_list = [] for key , value in resources . items (): if key not in resource_mappings : print ( f \"Warning: Unrecognized resource key: { key } . Skipping.\" ) continue slurm_option = resource_mappings . get ( key , key ) if value is True : # Flags without values resources_list . append ( f \" { slurm_option } \" ) else : resources_list . append ( f \" { slurm_option } = { value } \" ) resources_string = ' ' . join ( resources_list ) return resources_string sge_check_job_status ( job_ids ) Check if SGE jobs are still running or have completed. Parameters: job_ids ( list ) \u2013 A list of job IDs (as integers or strings) Returns: dict \u2013 A dictionary with job IDs as keys and statuses as values ('running' or 'completed') Source code in ribbon/batch/queue_utils.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def sge_check_job_status ( job_ids ): \"\"\" Check if SGE jobs are still running or have completed. Parameters: job_ids (list): A list of job IDs (as integers or strings) Returns: dict: A dictionary with job IDs as keys and statuses as values ('running' or 'completed') \"\"\" status_dict = {} for jobid in job_ids : try : # Run 'qstat -j <jobid>' and suppress output result = subprocess . run ( [ 'qstat' , '-j' , str ( jobid )], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) if result . returncode == 0 : status = 'not completed' else : status = 'completed' status_dict [ jobid ] = status except Exception as e : status_dict [ jobid ] = f 'Error: { e } ' return status_dict slurm_check_job_status ( job_ids ) Check if SLURM jobs are still running or have completed. Parameters: job_ids ( list ) \u2013 A list of job IDs (as integers or strings) Returns: dict \u2013 A dictionary with job IDs as keys and statuses as values ('running' or 'completed') Source code in ribbon/batch/queue_utils.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def slurm_check_job_status ( job_ids ): \"\"\" Check if SLURM jobs are still running or have completed. Parameters: job_ids (list): A list of job IDs (as integers or strings) Returns: dict: A dictionary with job IDs as keys and statuses as values ('running' or 'completed') \"\"\" status_dict = {} for jobid in job_ids : try : # Run 'squeue -j <jobid> --noheader' and capture the output. result = subprocess . run ( [ 'squeue' , '-j' , str ( jobid ), '--noheader' ], stdout = subprocess . PIPE , stderr = subprocess . DEVNULL , text = True ) # If any output is returned, the job is still in the queue (running or pending) if result . stdout . strip (): status = 'not completed' else : status = 'completed' status_dict [ jobid ] = status except Exception as e : status_dict [ jobid ] = f 'Error: { e } ' return status_dict","title":"Batch Processing"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.generate_sge_command","text":"Generate an SGE command to submit a job to the scheduler. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. other_resources ( dict ) \u2013 A dictionary of other resources to pass to the scheduler. job_variables ( str ) \u2013 A string of environment variables to pass to the job. scheduler_script ( str ) \u2013 The path to the script to run. Returns: str \u2013 The SGE command to submit Source code in ribbon/batch/queue_utils.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def generate_sge_command ( resources , other_resources , job_variables , scheduler_script ): \"\"\" Generate an SGE command to submit a job to the scheduler. Args: resources (dict): A dictionary of resources to request for the job. other_resources (dict): A dictionary of other resources to pass to the scheduler. job_variables (str): A string of environment variables to pass to the job. scheduler_script (str): The path to the script to run. Returns: str: The SGE command to submit \"\"\" scheduler_command = 'qsub' # Map resources to SGE options resources_string = parse_sge_resources ( resources ) # Add other resources as-is, from dict: for key , value in other_resources . items (): if value == '' : # If value is empty, assume it's a flag without a value resources_string += f \" { key } \" else : if key . startswith ( '-l' ): resources_string += f \" { key } = { value } \" else : resources_string += f \" { key } { value } \" # Construct the command command = f \" { scheduler_command } -v { job_variables } { resources_string } { scheduler_script } \" return command","title":"generate_sge_command"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.generate_slurm_command","text":"Generate a SLURM command to submit a job to the scheduler. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. other_resources ( dict ) \u2013 A dictionary of other resources to pass to the scheduler. job_variables ( str ) \u2013 A string of environment variables to pass to the job. scheduler_script ( str ) \u2013 The path to the script to run. Returns: str \u2013 The SLURM command to submit Source code in ribbon/batch/queue_utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ): \"\"\" Generate a SLURM command to submit a job to the scheduler. Args: resources (dict): A dictionary of resources to request for the job. other_resources (dict): A dictionary of other resources to pass to the scheduler. job_variables (str): A string of environment variables to pass to the job. scheduler_script (str): The path to the script to run. Returns: str: The SLURM command to submit \"\"\" scheduler_command = 'sbatch' # Map resources to SLURM options resources_string = parse_slurm_resources ( resources ) # Add other resources as-is, from dict: for key , value in other_resources . items (): if value == '' : # If value is empty, assume it's a flag without a value resources_string += f \" { key } \" else : resources_string += f \" { key } = { value } \" # Construct the command command = f \" { scheduler_command } --export= { job_variables } { resources_string } { scheduler_script } \" return command","title":"generate_slurm_command"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.parse_sge_resources","text":"Parse a dictionary of resources into a string of SGE options. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. Returns: str \u2013 A string of SGE options TODO: implement dependency handling Source code in ribbon/batch/queue_utils.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def parse_sge_resources ( resources , dependency_type = None ): \"\"\" Parse a dictionary of resources into a string of SGE options. Args: resources (dict): A dictionary of resources to request for the job. Returns: str: A string of SGE options TODO: implement dependency handling \"\"\" resource_mappings = { 'time' : '-l h_rt' , 'mem' : '-l mem_free' , 'dependency' : '-hold_jid' , 'gpus' : '-l gpu' , 'job-name' : '-N' , 'output' : '-o' , 'queue' : '-q' , 'node-name' : '-l hostname' , # Add other resource mappings as needed } # Parse dependencies: if 'dependency' in resources : dependencies = resources [ 'dependency' ] if isinstance ( dependencies , list ): dependencies = ',' . join ([ str ( job_id ) for job_id in dependencies ]) resources [ 'dependency' ] = dependencies resources_list = [] for key , value in resources . items (): if key == 'dependency' : # Handle dependencies specifically resources_list . append ( f \"-hold_jid { value } \" ) else : if key not in resource_mappings : print ( f \"Warning: Unrecognized resource key: { key } . Skipping.\" ) continue sge_option = resource_mappings . get ( key ) if sge_option : if sge_option . startswith ( '-l' ): resources_list . append ( f \" { sge_option } = { value } \" ) else : resources_list . append ( f \" { sge_option } { value } \" ) else : # For unrecognized keys, assume they are '-l key=value' resources_list . append ( f \"-l { key } = { value } \" ) resources_string = ' ' . join ( resources_list ) return resources_string","title":"parse_sge_resources"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.parse_slurm_resources","text":"Parse a dictionary of resources into a string of SLURM options. Parameters: resources ( dict ) \u2013 A dictionary of resources to request for the job. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency to use (e.g. 'afterok', 'afterany', 'afternotok') Returns: str \u2013 A string of SLURM options Source code in ribbon/batch/queue_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def parse_slurm_resources ( resources , dependency_type = 'afterok' ): \"\"\" Parse a dictionary of resources into a string of SLURM options. Args: resources (dict): A dictionary of resources to request for the job. dependency_type (str): The type of dependency to use (e.g. 'afterok', 'afterany', 'afternotok') Returns: str: A string of SLURM options \"\"\" resource_mappings = { 'time' : '--time' , 'mem' : '--mem' , 'dependency' : '--dependency' , 'gpus' : '--gpus' , 'job-name' : '--job-name' , 'requeue' : '--requeue' , 'output' : '--output' , 'queue' : '--partition' , 'node-name' : '--nodelist' , # Add other resource mappings as needed } # Parse dependencies: if 'dependency' in resources : dependencies = resources [ 'dependency' ] if isinstance ( dependencies , list ): dependencies = ':' . join ([ str ( job_id ) for job_id in dependencies ]) resources [ 'dependency' ] = dependency_type + ':' + dependencies resources_list = [] for key , value in resources . items (): if key not in resource_mappings : print ( f \"Warning: Unrecognized resource key: { key } . Skipping.\" ) continue slurm_option = resource_mappings . get ( key , key ) if value is True : # Flags without values resources_list . append ( f \" { slurm_option } \" ) else : resources_list . append ( f \" { slurm_option } = { value } \" ) resources_string = ' ' . join ( resources_list ) return resources_string","title":"parse_slurm_resources"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.sge_check_job_status","text":"Check if SGE jobs are still running or have completed. Parameters: job_ids ( list ) \u2013 A list of job IDs (as integers or strings) Returns: dict \u2013 A dictionary with job IDs as keys and statuses as values ('running' or 'completed') Source code in ribbon/batch/queue_utils.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def sge_check_job_status ( job_ids ): \"\"\" Check if SGE jobs are still running or have completed. Parameters: job_ids (list): A list of job IDs (as integers or strings) Returns: dict: A dictionary with job IDs as keys and statuses as values ('running' or 'completed') \"\"\" status_dict = {} for jobid in job_ids : try : # Run 'qstat -j <jobid>' and suppress output result = subprocess . run ( [ 'qstat' , '-j' , str ( jobid )], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) if result . returncode == 0 : status = 'not completed' else : status = 'completed' status_dict [ jobid ] = status except Exception as e : status_dict [ jobid ] = f 'Error: { e } ' return status_dict","title":"sge_check_job_status"},{"location":"api/batch/batch/#ribbon.batch.queue_utils.slurm_check_job_status","text":"Check if SLURM jobs are still running or have completed. Parameters: job_ids ( list ) \u2013 A list of job IDs (as integers or strings) Returns: dict \u2013 A dictionary with job IDs as keys and statuses as values ('running' or 'completed') Source code in ribbon/batch/queue_utils.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def slurm_check_job_status ( job_ids ): \"\"\" Check if SLURM jobs are still running or have completed. Parameters: job_ids (list): A list of job IDs (as integers or strings) Returns: dict: A dictionary with job IDs as keys and statuses as values ('running' or 'completed') \"\"\" status_dict = {} for jobid in job_ids : try : # Run 'squeue -j <jobid> --noheader' and capture the output. result = subprocess . run ( [ 'squeue' , '-j' , str ( jobid ), '--noheader' ], stdout = subprocess . PIPE , stderr = subprocess . DEVNULL , text = True ) # If any output is returned, the job is still in the queue (running or pending) if result . stdout . strip (): status = 'not completed' else : status = 'completed' status_dict [ jobid ] = status except Exception as e : status_dict [ jobid ] = f 'Error: { e } ' return status_dict","title":"slurm_check_job_status"},{"location":"api/ribbon_tasks/ribbon_tasks/","text":"CalculateDistance Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 class CalculateDistance ( Task ): def __init__ ( self , pdb_file , chain1_id , res1_id , atom1_name , chain2_id , res2_id , atom2_name , output_file , device = 'cpu' ): \"\"\" Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. chain1_id (str): Chain ID of the first atom. res1_id (str): Residue ID of the first atom. atom1_name (str): Name of the first atom. chain2_id (str): Chain ID of the second atom. res2_id (str): Residue ID of the second atom. atom2_name (str): Name of the second atom. output_file (str): Path to the output file. Suffixed with '.dist'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate Distance\" # Task-specific variables self . pdb_file = pdb_file self . chain1_id = chain1_id self . res1_id = res1_id self . atom1_name = atom1_name self . chain2_id = chain2_id self . res2_id = res2_id self . atom2_name = atom2_name self . output_file = output_file self . device = device def run ( self ): # Ensure output directory exists make_directory ( Path ( self . output_file ) . parent ) # Run the task self . _run_task ( self . task_name , pdb_file = self . pdb_file , chain1_id = self . chain1_id , res1_id = self . res1_id , atom1_name = self . atom1_name , chain2_id = self . chain2_id , res2_id = self . res2_id , atom2_name = self . atom2_name , output_file = self . output_file , device = self . device ) __init__ ( pdb_file , chain1_id , res1_id , atom1_name , chain2_id , res2_id , atom2_name , output_file , device = 'cpu' ) Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Parameters: pdb_file ( str ) \u2013 Path to the PDB file. chain1_id ( str ) \u2013 Chain ID of the first atom. res1_id ( str ) \u2013 Residue ID of the first atom. atom1_name ( str ) \u2013 Name of the first atom. chain2_id ( str ) \u2013 Chain ID of the second atom. res2_id ( str ) \u2013 Residue ID of the second atom. atom2_name ( str ) \u2013 Name of the second atom. output_file ( str ) \u2013 Path to the output file. Suffixed with '.dist'. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def __init__ ( self , pdb_file , chain1_id , res1_id , atom1_name , chain2_id , res2_id , atom2_name , output_file , device = 'cpu' ): \"\"\" Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. chain1_id (str): Chain ID of the first atom. res1_id (str): Residue ID of the first atom. atom1_name (str): Name of the first atom. chain2_id (str): Chain ID of the second atom. res2_id (str): Residue ID of the second atom. atom2_name (str): Name of the second atom. output_file (str): Path to the output file. Suffixed with '.dist'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate Distance\" # Task-specific variables self . pdb_file = pdb_file self . chain1_id = chain1_id self . res1_id = res1_id self . atom1_name = atom1_name self . chain2_id = chain2_id self . res2_id = res2_id self . atom2_name = atom2_name self . output_file = output_file self . device = device CalculateSASA Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class CalculateSASA ( Task ): def __init__ ( self , pdb_file , output_file , atom_1 , device = 'cpu' ): \"\"\" Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. output_file (str): Path to the output file. Suffixed with '.angle'. atom_1 (str): Atom specification in format chain_id:res_id:atom_name. device (str): The device to run the task on. Default is 'cpu'. TODO: - Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py \"\"\" raise NotImplementedError ( 'This task is not yet implemented .' ) # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate SASA\" # Task-specific variables self . pdb_file = pdb_file self . output_file = output_file self . atom_1 = atom_1 self . device = device def run ( self ): # Ensure output directory exists make_directory ( Path ( self . output_file ) . parent ) # Run the task self . _run_task ( self . task_name , pdb_file = self . pdb_file , output_file = self . output_file , atom_1 = self . atom_1 , device = self . device ) __init__ ( pdb_file , output_file , atom_1 , device = 'cpu' ) Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Parameters: pdb_file ( str ) \u2013 Path to the PDB file. output_file ( str ) \u2013 Path to the output file. Suffixed with '.angle'. atom_1 ( str ) \u2013 Atom specification in format chain_id:res_id:atom_name. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. TODO Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def __init__ ( self , pdb_file , output_file , atom_1 , device = 'cpu' ): \"\"\" Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. output_file (str): Path to the output file. Suffixed with '.angle'. atom_1 (str): Atom specification in format chain_id:res_id:atom_name. device (str): The device to run the task on. Default is 'cpu'. TODO: - Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py \"\"\" raise NotImplementedError ( 'This task is not yet implemented .' ) # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate SASA\" # Task-specific variables self . pdb_file = pdb_file self . output_file = output_file self . atom_1 = atom_1 self . device = device Chai1 Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class Chai1 ( Task ): def __init__ ( self , fasta_file , output_dir = '.' , smiles_string = None , num_ligands = 1 , device = 'gpu' ): \"\"\" Initialize a Chai-1 task. Args: fasta_file (str): The FASTA file containing the protein sequence (no ligand). output_dir (str): The directory to save the output files. Default is '.'. smiles_string (str, optional): The SMILES string of the ligand. Default is None. num_ligands (int): The number of ligands. Default is 1. device (str): The device to run the task on. Default is 'gpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Chai-1\" # Task-specific variables self . fasta_file = fasta_file self . smiles_string = smiles_string self . output_dir = output_dir self . device = device self . num_ligands = num_ligands def run ( self ): # Make the directory: self . output_dir = make_directory ( self . output_dir ) # Run the task self . _run_task ( self . task_name , fasta_file = self . fasta_file , smiles_string = self . smiles_string , output_dir = str ( self . output_dir ), num_ligands = self . num_ligands , device = self . device ) __init__ ( fasta_file , output_dir = '.' , smiles_string = None , num_ligands = 1 , device = 'gpu' ) Initialize a Chai-1 task. Parameters: fasta_file ( str ) \u2013 The FASTA file containing the protein sequence (no ligand). output_dir ( str , default: '.' ) \u2013 The directory to save the output files. Default is '.'. smiles_string ( str , default: None ) \u2013 The SMILES string of the ligand. Default is None. num_ligands ( int , default: 1 ) \u2013 The number of ligands. Default is 1. device ( str , default: 'gpu' ) \u2013 The device to run the task on. Default is 'gpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , fasta_file , output_dir = '.' , smiles_string = None , num_ligands = 1 , device = 'gpu' ): \"\"\" Initialize a Chai-1 task. Args: fasta_file (str): The FASTA file containing the protein sequence (no ligand). output_dir (str): The directory to save the output files. Default is '.'. smiles_string (str, optional): The SMILES string of the ligand. Default is None. num_ligands (int): The number of ligands. Default is 1. device (str): The device to run the task on. Default is 'gpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Chai-1\" # Task-specific variables self . fasta_file = fasta_file self . smiles_string = smiles_string self . output_dir = output_dir self . device = device self . num_ligands = num_ligands Custom Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 class Custom ( Task ): def __init__ ( self , command , container = 'Ribbon' , device = 'cpu' ): \"\"\" Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Args: command (str): The command to run. container (str): The container to run the command in. Default is 'Ribbon'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Custom\" # Task-specific variables self . command = command self . container = container self . device = device def run ( self ): # Run the task self . _run_task ( self . task_name , command = self . command , container_override = self . container , device = self . device ) __init__ ( command , container = 'Ribbon' , device = 'cpu' ) Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Parameters: command ( str ) \u2013 The command to run. container ( str , default: 'Ribbon' ) \u2013 The container to run the command in. Default is 'Ribbon'. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def __init__ ( self , command , container = 'Ribbon' , device = 'cpu' ): \"\"\" Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Args: command (str): The command to run. container (str): The container to run the command in. Default is 'Ribbon'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Custom\" # Task-specific variables self . command = command self . container = container self . device = device FastRelax Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class FastRelax ( Task ): def __init__ ( self , output_dir , pdb_input_file = None , pdb_input_dir = None , device = 'cpu' ): \"\"\" Initialize a FastRelax task. Args: output_dir (str): The directory to save the output files. pdb_input_file (str, optional): Path to a single PDB file. Default is None. pdb_input_dir (str, optional): Path to a directory containing PDB files. Default is None. device (str): The device to run the task on. Default is 'cpu'. Raises: ValueError: If neither pdb_input_file nor pdb_input_dir is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"FastRelax\" # Task-specific variables if pdb_input_file is None and pdb_input_dir is None : raise ValueError ( 'Must specify either pdb_input_file or pdb_input_dir' ) self . output_dir = make_directory ( output_dir ) self . pdb_input_file = pdb_input_file self . pdb_input_dir = pdb_input_dir self . device = device def run ( self ): # Handle input files if self . pdb_input_file is not None : temp_dir = tempfile . mkdtemp () shutil . copy ( self . pdb_input_file , temp_dir ) pdb_input_dir = temp_dir else : pdb_input_dir = self . pdb_input_dir # Prepare PDB files pdb_list = list_files ( pdb_input_dir , '.pdb' ) pdb_string = \" \" . join ( map ( str , pdb_list )) + \" \" # Run the task self . _run_task ( self . task_name , pdb_string = pdb_string , output_dir = str ( self . output_dir ), device = self . device ) __init__ ( output_dir , pdb_input_file = None , pdb_input_dir = None , device = 'cpu' ) Initialize a FastRelax task. Parameters: output_dir ( str ) \u2013 The directory to save the output files. pdb_input_file ( str , default: None ) \u2013 Path to a single PDB file. Default is None. pdb_input_dir ( str , default: None ) \u2013 Path to a directory containing PDB files. Default is None. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Raises: ValueError \u2013 If neither pdb_input_file nor pdb_input_dir is specified. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , output_dir , pdb_input_file = None , pdb_input_dir = None , device = 'cpu' ): \"\"\" Initialize a FastRelax task. Args: output_dir (str): The directory to save the output files. pdb_input_file (str, optional): Path to a single PDB file. Default is None. pdb_input_dir (str, optional): Path to a directory containing PDB files. Default is None. device (str): The device to run the task on. Default is 'cpu'. Raises: ValueError: If neither pdb_input_file nor pdb_input_dir is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"FastRelax\" # Task-specific variables if pdb_input_file is None and pdb_input_dir is None : raise ValueError ( 'Must specify either pdb_input_file or pdb_input_dir' ) self . output_dir = make_directory ( output_dir ) self . pdb_input_file = pdb_input_file self . pdb_input_dir = pdb_input_dir self . device = device LigandMPNN Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class LigandMPNN ( Task ): def __init__ ( self , output_dir , structure_list , num_designs = 1 , device = 'cpu' , extra_args = \"\" ): \"\"\" Initialize a LigandMPNN task. Args: output_dir (str): The directory to save the output files. structure_list (list): A list of pdb or cif files to use as input structures. num_designs (int): The number of designs to generate per input structure. device (str): The device to run the task on. Default is 'cpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Returns: None Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. \"\"\" # Initialize the Task class super () . __init__ ( device = device , extra_args = extra_args ) # This Task name matches the name in the tasks.json file self . task_name = \"LigandMPNN\" # Your arguments here: self . output_dir = output_dir self . structure_list = structure_list self . num_designs = num_designs def run ( self ): ###### HELPER FUNCTIONS ####### def split_ligandmpnn_fasta ( fasta_file , split_output_dir ): # Each LigandMPNN input produces a FASTA with multiple outputs as > lines. Each line has 1 or more chains separated by ':'. # Here, we separate each output into it's own FASTA file, with chains as separate > lines. split_output_dir . mkdir ( parents = True , exist_ok = True ) with open ( fasta_file ) as f : # Skip the first two lines (original input) lines = f . readlines ()[ 2 :] for i in range ( 0 , len ( lines ), 2 ): if not lines [ i ] . startswith ( '>' ): continue name = lines [ i ] . strip () chains = lines [ i + 1 ] . strip () . split ( ':' ) index = 0 output_path = split_output_dir / f ' { fasta_file . stem } _ { index } .fasta' # Increment the index to avoid overwriting existing files while output_path . exists (): index += 1 output_path = split_output_dir / f ' { fasta_file . stem } _ { index } .fasta' print ( f 'Writing to { output_path } ' ) with open ( output_path , 'w' ) as g : for chain_index , chain in enumerate ( chains ): name_with_chain = f \" { name . split ( ',' )[ 0 ] } _ { chain_index } \" + ', ' + ',' . join ( name . split ( ',' )[ 1 :]) # Add chain to name g . write ( f ' { name_with_chain } \\n { chain } \\n ' ) # Make directories: self . output_dir = make_directory ( self . output_dir ) # Then, write out the files within pdb_input_dir to a json file: #pdb_input_json = self.output_dir / 'pdb_input.json' # Make a temp file for the json: pdb_input_json = tempfile . NamedTemporaryFile ( delete = False ) . name with open ( pdb_input_json , 'w' ) as f : json . dump ( self . structure_list , f ) # Run the task: self . _run_task ( self . task_name , pdb_input_json = pdb_input_json , output_dir = self . output_dir , num_designs = self . num_designs , extra_args = self . extra_args , device = self . device ) # Split the FASTA files: for file in ( self . output_dir / 'seqs' ) . iterdir (): print ( f 'Splitting { file } ' ) split_ligandmpnn_fasta ( file , self . output_dir / 'seqs_split' ) return __init__ ( output_dir , structure_list , num_designs = 1 , device = 'cpu' , extra_args = '' ) Initialize a LigandMPNN task. Parameters: output_dir ( str ) \u2013 The directory to save the output files. structure_list ( list ) \u2013 A list of pdb or cif files to use as input structures. num_designs ( int , default: 1 ) \u2013 The number of designs to generate per input structure. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. extra_args ( str , default: '' ) \u2013 Additional arguments for the task. Default is an empty string. Returns: \u2013 None \u2013 Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , output_dir , structure_list , num_designs = 1 , device = 'cpu' , extra_args = \"\" ): \"\"\" Initialize a LigandMPNN task. Args: output_dir (str): The directory to save the output files. structure_list (list): A list of pdb or cif files to use as input structures. num_designs (int): The number of designs to generate per input structure. device (str): The device to run the task on. Default is 'cpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Returns: None Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. \"\"\" # Initialize the Task class super () . __init__ ( device = device , extra_args = extra_args ) # This Task name matches the name in the tasks.json file self . task_name = \"LigandMPNN\" # Your arguments here: self . output_dir = output_dir self . structure_list = structure_list self . num_designs = num_designs RaptorXSingle Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 class RaptorXSingle ( Task ): def __init__ ( self , fasta_file_or_dir , output_dir = '.' , param = 'RaptorX-Single-ESM1b.pt' , device = 'gpu' , extra_args = \"\" ): \"\"\" Initialize a RaptorXSingle task. Args: fasta_file_or_dir (str): The FASTA file or directory containing multiple FASTA files. output_dir (str): The directory to save the output files. Default is '.'. param (str): The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device (str): The device to run the task on. Default is 'gpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Raises: ValueError: If an invalid param is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"RaptorXSingle\" # Task-specific variables self . fasta_file = fasta_file_or_dir self . output_dir = output_dir self . param = 'RaptorX-Single/params/' + param #This is the directory where the params are stored self . extra_args = extra_args self . device = device if device == 'gpu' : self . device_id = '0' elif device == 'cpu' : self . device_id = '-1' else : self . device_id = str ( device ) self . device = 'cpu' if device == '-1' else 'gpu' # Check inputs: valid_param_list = [ 'RaptorX-Single-ESM1b.pt' , 'RaptorX-Single-ESM1v.pt' , 'RaptorX-Single-ProtTrans.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt' , 'RaptorX-Single-ESM1b-Ab.pt' , 'RaptorX-Single-ESM1v-Ab.pt' , 'RaptorX-Single-ProtTrans-Ab.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt' ] if param not in valid_param_list : raise ValueError ( f 'Invalid param: { param } . Must be one of { valid_param_list } ' ) def run ( self ): # Make the directory: self . output_dir = make_directory ( self . output_dir ) # \"python RaptorX-Single/pred.py {fasta_file} RaptorX-Single/params/{param} --plm_param_dir RaptorX-Single/params/ --out_dir {output_dir} --device {device} {extra_args}\", # Run the task self . _run_task ( self . task_name , fasta_file = self . fasta_file , param = self . param , output_dir = str ( self . output_dir ), device = self . device , device_id = self . device_id , extra_args = self . extra_args ) __init__ ( fasta_file_or_dir , output_dir = '.' , param = 'RaptorX-Single-ESM1b.pt' , device = 'gpu' , extra_args = '' ) Initialize a RaptorXSingle task. Parameters: fasta_file_or_dir ( str ) \u2013 The FASTA file or directory containing multiple FASTA files. output_dir ( str , default: '.' ) \u2013 The directory to save the output files. Default is '.'. param ( str , default: 'RaptorX-Single-ESM1b.pt' ) \u2013 The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device ( str , default: 'gpu' ) \u2013 The device to run the task on. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments for the task. Default is an empty string. Raises: ValueError \u2013 If an invalid param is specified. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , fasta_file_or_dir , output_dir = '.' , param = 'RaptorX-Single-ESM1b.pt' , device = 'gpu' , extra_args = \"\" ): \"\"\" Initialize a RaptorXSingle task. Args: fasta_file_or_dir (str): The FASTA file or directory containing multiple FASTA files. output_dir (str): The directory to save the output files. Default is '.'. param (str): The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device (str): The device to run the task on. Default is 'gpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Raises: ValueError: If an invalid param is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"RaptorXSingle\" # Task-specific variables self . fasta_file = fasta_file_or_dir self . output_dir = output_dir self . param = 'RaptorX-Single/params/' + param #This is the directory where the params are stored self . extra_args = extra_args self . device = device if device == 'gpu' : self . device_id = '0' elif device == 'cpu' : self . device_id = '-1' else : self . device_id = str ( device ) self . device = 'cpu' if device == '-1' else 'gpu' # Check inputs: valid_param_list = [ 'RaptorX-Single-ESM1b.pt' , 'RaptorX-Single-ESM1v.pt' , 'RaptorX-Single-ProtTrans.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt' , 'RaptorX-Single-ESM1b-Ab.pt' , 'RaptorX-Single-ESM1v-Ab.pt' , 'RaptorX-Single-ProtTrans-Ab.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt' ] if param not in valid_param_list : raise ValueError ( f 'Invalid param: { param } . Must be one of { valid_param_list } ' ) Task Source code in ribbon/runner.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Task : def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" ) def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \" __init__ ( device = 'cpu' , extra_args = '' ) The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Parameters: device ( str , default: 'cpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task Returns: \u2013 None Source code in ribbon/runner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None __repr__ () Returns a string representation of the Task object. Source code in ribbon/runner.py 220 221 222 223 224 225 def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \" _get_task_dict ( task_name ) Returns the dictionary for a given task. Source code in ribbon/runner.py 197 198 199 200 201 202 203 204 205 def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] _get_task_inputs ( task_name ) Returns the inputs required for a given task Source code in ribbon/runner.py 207 208 209 210 211 212 213 214 215 216 217 218 def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs _run_task ( task_name , scheduler = 'local' , device = 'gpu' , extra_args = '' , container_override = None , ** kwargs ) Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Parameters: task_name ( str ) \u2013 The name of the task to run. device ( str , default: 'gpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override ( str , default: None ) \u2013 The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs ( dict , default: {} ) \u2013 Task-specific keyword arguments. Returns: \u2013 None Source code in ribbon/runner.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) queue ( scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ) Queue the LigandMPNN task using the given scheduler. Parameters: scheduler ( str ) \u2013 The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on ( list , default: [] ) \u2013 A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks ( int , default: 1 ) \u2013 The number of tasks to run. Defaults to 1. time ( str , default: '1:00:00' ) \u2013 The time to allocate for the task. Defaults to '1:00:00'. mem ( str , default: '2G' ) \u2013 The memory to allocate for the task. Defaults to '2G'. auto_restart ( bool , default: True ) \u2013 Whether to automatically restart the task if it fails. Defaults to True. other_resources ( dict , default: {} ) \u2013 Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name ( str , default: None ) \u2013 The name of the job. Defaults to None. output_file ( str , default: None ) \u2013 The file to write the output to. Defaults to None. queue ( str , default: None ) \u2013 The queue to submit the task to. Defaults to None. gpus ( int , default: None ) \u2013 The number of GPUs to allocate for the task. Defaults to None. node_name ( str , default: None ) \u2013 The name of the node to run the task on. Defaults to None. Returns: str \u2013 The ID of the job in the scheduler. Source code in ribbon/runner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id run () Run the task. This method should be overridden by the child class. Source code in ribbon/runner.py 25 26 27 28 29 def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" ) list_files ( directory , extension ) Returns a list of files in a directory with a given extension Source code in ribbon/utils.py 15 16 17 def list_files ( directory , extension ): \"\"\"Returns a list of files in a directory with a given extension\"\"\" return [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if f . endswith ( extension )] make_directories ( * directories ) Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. Source code in ribbon/utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_directories ( * directories ): \"\"\" Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. \"\"\" new_directories = [] for directory in directories : # Check it's a Path object: if not isinstance ( directory , Path ): directory = Path ( directory ) directory . mkdir ( parents = True , exist_ok = True ) new_directories . append ( directory ) return new_directories make_directory ( directory ) Creates a directory if it does not exist. Parameters: directory ( str or Path ) \u2013 The directory to create. Returns: Path \u2013 path object of the created directory. Source code in ribbon/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def make_directory ( directory ): \"\"\" Creates a directory if it does not exist. Args: directory (str or Path): The directory to create. Returns: Path: path object of the created directory. \"\"\" directory = make_directories ( directory )[ 0 ] return directory","title":"Ribbon Tasks"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.CalculateDistance","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 class CalculateDistance ( Task ): def __init__ ( self , pdb_file , chain1_id , res1_id , atom1_name , chain2_id , res2_id , atom2_name , output_file , device = 'cpu' ): \"\"\" Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. chain1_id (str): Chain ID of the first atom. res1_id (str): Residue ID of the first atom. atom1_name (str): Name of the first atom. chain2_id (str): Chain ID of the second atom. res2_id (str): Residue ID of the second atom. atom2_name (str): Name of the second atom. output_file (str): Path to the output file. Suffixed with '.dist'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate Distance\" # Task-specific variables self . pdb_file = pdb_file self . chain1_id = chain1_id self . res1_id = res1_id self . atom1_name = atom1_name self . chain2_id = chain2_id self . res2_id = res2_id self . atom2_name = atom2_name self . output_file = output_file self . device = device def run ( self ): # Ensure output directory exists make_directory ( Path ( self . output_file ) . parent ) # Run the task self . _run_task ( self . task_name , pdb_file = self . pdb_file , chain1_id = self . chain1_id , res1_id = self . res1_id , atom1_name = self . atom1_name , chain2_id = self . chain2_id , res2_id = self . res2_id , atom2_name = self . atom2_name , output_file = self . output_file , device = self . device )","title":"CalculateDistance"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.CalculateDistance.__init__","text":"Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Parameters: pdb_file ( str ) \u2013 Path to the PDB file. chain1_id ( str ) \u2013 Chain ID of the first atom. res1_id ( str ) \u2013 Residue ID of the first atom. atom1_name ( str ) \u2013 Name of the first atom. chain2_id ( str ) \u2013 Chain ID of the second atom. res2_id ( str ) \u2013 Residue ID of the second atom. atom2_name ( str ) \u2013 Name of the second atom. output_file ( str ) \u2013 Path to the output file. Suffixed with '.dist'. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def __init__ ( self , pdb_file , chain1_id , res1_id , atom1_name , chain2_id , res2_id , atom2_name , output_file , device = 'cpu' ): \"\"\" Initialize a CalculateDistance task. This calculates the distance between two atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. chain1_id (str): Chain ID of the first atom. res1_id (str): Residue ID of the first atom. atom1_name (str): Name of the first atom. chain2_id (str): Chain ID of the second atom. res2_id (str): Residue ID of the second atom. atom2_name (str): Name of the second atom. output_file (str): Path to the output file. Suffixed with '.dist'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate Distance\" # Task-specific variables self . pdb_file = pdb_file self . chain1_id = chain1_id self . res1_id = res1_id self . atom1_name = atom1_name self . chain2_id = chain2_id self . res2_id = res2_id self . atom2_name = atom2_name self . output_file = output_file self . device = device","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.CalculateSASA","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class CalculateSASA ( Task ): def __init__ ( self , pdb_file , output_file , atom_1 , device = 'cpu' ): \"\"\" Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. output_file (str): Path to the output file. Suffixed with '.angle'. atom_1 (str): Atom specification in format chain_id:res_id:atom_name. device (str): The device to run the task on. Default is 'cpu'. TODO: - Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py \"\"\" raise NotImplementedError ( 'This task is not yet implemented .' ) # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate SASA\" # Task-specific variables self . pdb_file = pdb_file self . output_file = output_file self . atom_1 = atom_1 self . device = device def run ( self ): # Ensure output directory exists make_directory ( Path ( self . output_file ) . parent ) # Run the task self . _run_task ( self . task_name , pdb_file = self . pdb_file , output_file = self . output_file , atom_1 = self . atom_1 , device = self . device )","title":"CalculateSASA"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.CalculateSASA.__init__","text":"Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Parameters: pdb_file ( str ) \u2013 Path to the PDB file. output_file ( str ) \u2013 Path to the output file. Suffixed with '.angle'. atom_1 ( str ) \u2013 Atom specification in format chain_id:res_id:atom_name. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. TODO Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def __init__ ( self , pdb_file , output_file , atom_1 , device = 'cpu' ): \"\"\" Initialize a CalculateSASA task. This calculates the Solvent Accessible Surface Area for a set of atoms in a PDB file. Args: pdb_file (str): Path to the PDB file. output_file (str): Path to the output file. Suffixed with '.angle'. atom_1 (str): Atom specification in format chain_id:res_id:atom_name. device (str): The device to run the task on. Default is 'cpu'. TODO: - Implement the task script in ribbon/ribbon_tasks/task_scripts/calculate_sasa.py \"\"\" raise NotImplementedError ( 'This task is not yet implemented .' ) # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Calculate SASA\" # Task-specific variables self . pdb_file = pdb_file self . output_file = output_file self . atom_1 = atom_1 self . device = device","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Chai1","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class Chai1 ( Task ): def __init__ ( self , fasta_file , output_dir = '.' , smiles_string = None , num_ligands = 1 , device = 'gpu' ): \"\"\" Initialize a Chai-1 task. Args: fasta_file (str): The FASTA file containing the protein sequence (no ligand). output_dir (str): The directory to save the output files. Default is '.'. smiles_string (str, optional): The SMILES string of the ligand. Default is None. num_ligands (int): The number of ligands. Default is 1. device (str): The device to run the task on. Default is 'gpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Chai-1\" # Task-specific variables self . fasta_file = fasta_file self . smiles_string = smiles_string self . output_dir = output_dir self . device = device self . num_ligands = num_ligands def run ( self ): # Make the directory: self . output_dir = make_directory ( self . output_dir ) # Run the task self . _run_task ( self . task_name , fasta_file = self . fasta_file , smiles_string = self . smiles_string , output_dir = str ( self . output_dir ), num_ligands = self . num_ligands , device = self . device )","title":"Chai1"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Chai1.__init__","text":"Initialize a Chai-1 task. Parameters: fasta_file ( str ) \u2013 The FASTA file containing the protein sequence (no ligand). output_dir ( str , default: '.' ) \u2013 The directory to save the output files. Default is '.'. smiles_string ( str , default: None ) \u2013 The SMILES string of the ligand. Default is None. num_ligands ( int , default: 1 ) \u2013 The number of ligands. Default is 1. device ( str , default: 'gpu' ) \u2013 The device to run the task on. Default is 'gpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , fasta_file , output_dir = '.' , smiles_string = None , num_ligands = 1 , device = 'gpu' ): \"\"\" Initialize a Chai-1 task. Args: fasta_file (str): The FASTA file containing the protein sequence (no ligand). output_dir (str): The directory to save the output files. Default is '.'. smiles_string (str, optional): The SMILES string of the ligand. Default is None. num_ligands (int): The number of ligands. Default is 1. device (str): The device to run the task on. Default is 'gpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Chai-1\" # Task-specific variables self . fasta_file = fasta_file self . smiles_string = smiles_string self . output_dir = output_dir self . device = device self . num_ligands = num_ligands","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Custom","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 class Custom ( Task ): def __init__ ( self , command , container = 'Ribbon' , device = 'cpu' ): \"\"\" Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Args: command (str): The command to run. container (str): The container to run the command in. Default is 'Ribbon'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Custom\" # Task-specific variables self . command = command self . container = container self . device = device def run ( self ): # Run the task self . _run_task ( self . task_name , command = self . command , container_override = self . container , device = self . device )","title":"Custom"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Custom.__init__","text":"Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Parameters: command ( str ) \u2013 The command to run. container ( str , default: 'Ribbon' ) \u2013 The container to run the command in. Default is 'Ribbon'. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def __init__ ( self , command , container = 'Ribbon' , device = 'cpu' ): \"\"\" Initialize a Custom task. This allows the user to run a custom command in a specified container. N.B. This allows the user to run arbitrary code; use with caution. Args: command (str): The command to run. container (str): The container to run the command in. Default is 'Ribbon'. device (str): The device to run the task on. Default is 'cpu'. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"Custom\" # Task-specific variables self . command = command self . container = container self . device = device","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.FastRelax","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class FastRelax ( Task ): def __init__ ( self , output_dir , pdb_input_file = None , pdb_input_dir = None , device = 'cpu' ): \"\"\" Initialize a FastRelax task. Args: output_dir (str): The directory to save the output files. pdb_input_file (str, optional): Path to a single PDB file. Default is None. pdb_input_dir (str, optional): Path to a directory containing PDB files. Default is None. device (str): The device to run the task on. Default is 'cpu'. Raises: ValueError: If neither pdb_input_file nor pdb_input_dir is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"FastRelax\" # Task-specific variables if pdb_input_file is None and pdb_input_dir is None : raise ValueError ( 'Must specify either pdb_input_file or pdb_input_dir' ) self . output_dir = make_directory ( output_dir ) self . pdb_input_file = pdb_input_file self . pdb_input_dir = pdb_input_dir self . device = device def run ( self ): # Handle input files if self . pdb_input_file is not None : temp_dir = tempfile . mkdtemp () shutil . copy ( self . pdb_input_file , temp_dir ) pdb_input_dir = temp_dir else : pdb_input_dir = self . pdb_input_dir # Prepare PDB files pdb_list = list_files ( pdb_input_dir , '.pdb' ) pdb_string = \" \" . join ( map ( str , pdb_list )) + \" \" # Run the task self . _run_task ( self . task_name , pdb_string = pdb_string , output_dir = str ( self . output_dir ), device = self . device )","title":"FastRelax"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.FastRelax.__init__","text":"Initialize a FastRelax task. Parameters: output_dir ( str ) \u2013 The directory to save the output files. pdb_input_file ( str , default: None ) \u2013 Path to a single PDB file. Default is None. pdb_input_dir ( str , default: None ) \u2013 Path to a directory containing PDB files. Default is None. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. Raises: ValueError \u2013 If neither pdb_input_file nor pdb_input_dir is specified. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , output_dir , pdb_input_file = None , pdb_input_dir = None , device = 'cpu' ): \"\"\" Initialize a FastRelax task. Args: output_dir (str): The directory to save the output files. pdb_input_file (str, optional): Path to a single PDB file. Default is None. pdb_input_dir (str, optional): Path to a directory containing PDB files. Default is None. device (str): The device to run the task on. Default is 'cpu'. Raises: ValueError: If neither pdb_input_file nor pdb_input_dir is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"FastRelax\" # Task-specific variables if pdb_input_file is None and pdb_input_dir is None : raise ValueError ( 'Must specify either pdb_input_file or pdb_input_dir' ) self . output_dir = make_directory ( output_dir ) self . pdb_input_file = pdb_input_file self . pdb_input_dir = pdb_input_dir self . device = device","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.LigandMPNN","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class LigandMPNN ( Task ): def __init__ ( self , output_dir , structure_list , num_designs = 1 , device = 'cpu' , extra_args = \"\" ): \"\"\" Initialize a LigandMPNN task. Args: output_dir (str): The directory to save the output files. structure_list (list): A list of pdb or cif files to use as input structures. num_designs (int): The number of designs to generate per input structure. device (str): The device to run the task on. Default is 'cpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Returns: None Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. \"\"\" # Initialize the Task class super () . __init__ ( device = device , extra_args = extra_args ) # This Task name matches the name in the tasks.json file self . task_name = \"LigandMPNN\" # Your arguments here: self . output_dir = output_dir self . structure_list = structure_list self . num_designs = num_designs def run ( self ): ###### HELPER FUNCTIONS ####### def split_ligandmpnn_fasta ( fasta_file , split_output_dir ): # Each LigandMPNN input produces a FASTA with multiple outputs as > lines. Each line has 1 or more chains separated by ':'. # Here, we separate each output into it's own FASTA file, with chains as separate > lines. split_output_dir . mkdir ( parents = True , exist_ok = True ) with open ( fasta_file ) as f : # Skip the first two lines (original input) lines = f . readlines ()[ 2 :] for i in range ( 0 , len ( lines ), 2 ): if not lines [ i ] . startswith ( '>' ): continue name = lines [ i ] . strip () chains = lines [ i + 1 ] . strip () . split ( ':' ) index = 0 output_path = split_output_dir / f ' { fasta_file . stem } _ { index } .fasta' # Increment the index to avoid overwriting existing files while output_path . exists (): index += 1 output_path = split_output_dir / f ' { fasta_file . stem } _ { index } .fasta' print ( f 'Writing to { output_path } ' ) with open ( output_path , 'w' ) as g : for chain_index , chain in enumerate ( chains ): name_with_chain = f \" { name . split ( ',' )[ 0 ] } _ { chain_index } \" + ', ' + ',' . join ( name . split ( ',' )[ 1 :]) # Add chain to name g . write ( f ' { name_with_chain } \\n { chain } \\n ' ) # Make directories: self . output_dir = make_directory ( self . output_dir ) # Then, write out the files within pdb_input_dir to a json file: #pdb_input_json = self.output_dir / 'pdb_input.json' # Make a temp file for the json: pdb_input_json = tempfile . NamedTemporaryFile ( delete = False ) . name with open ( pdb_input_json , 'w' ) as f : json . dump ( self . structure_list , f ) # Run the task: self . _run_task ( self . task_name , pdb_input_json = pdb_input_json , output_dir = self . output_dir , num_designs = self . num_designs , extra_args = self . extra_args , device = self . device ) # Split the FASTA files: for file in ( self . output_dir / 'seqs' ) . iterdir (): print ( f 'Splitting { file } ' ) split_ligandmpnn_fasta ( file , self . output_dir / 'seqs_split' ) return","title":"LigandMPNN"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.LigandMPNN.__init__","text":"Initialize a LigandMPNN task. Parameters: output_dir ( str ) \u2013 The directory to save the output files. structure_list ( list ) \u2013 A list of pdb or cif files to use as input structures. num_designs ( int , default: 1 ) \u2013 The number of designs to generate per input structure. device ( str , default: 'cpu' ) \u2013 The device to run the task on. Default is 'cpu'. extra_args ( str , default: '' ) \u2013 Additional arguments for the task. Default is an empty string. Returns: \u2013 None \u2013 Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , output_dir , structure_list , num_designs = 1 , device = 'cpu' , extra_args = \"\" ): \"\"\" Initialize a LigandMPNN task. Args: output_dir (str): The directory to save the output files. structure_list (list): A list of pdb or cif files to use as input structures. num_designs (int): The number of designs to generate per input structure. device (str): The device to run the task on. Default is 'cpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Returns: None Outputs the following directories: output_dir: - backbones: The generated backbones - packed: The packed structures, including sidechains - sequences: The generated sequences as FASTA files. Multiple chains are separated by ':'. Each line is a different design. - seqs_split: The sequences split into separate FASTA files, one per design. Each line is a different chain. \"\"\" # Initialize the Task class super () . __init__ ( device = device , extra_args = extra_args ) # This Task name matches the name in the tasks.json file self . task_name = \"LigandMPNN\" # Your arguments here: self . output_dir = output_dir self . structure_list = structure_list self . num_designs = num_designs","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.RaptorXSingle","text":"Bases: Task Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 class RaptorXSingle ( Task ): def __init__ ( self , fasta_file_or_dir , output_dir = '.' , param = 'RaptorX-Single-ESM1b.pt' , device = 'gpu' , extra_args = \"\" ): \"\"\" Initialize a RaptorXSingle task. Args: fasta_file_or_dir (str): The FASTA file or directory containing multiple FASTA files. output_dir (str): The directory to save the output files. Default is '.'. param (str): The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device (str): The device to run the task on. Default is 'gpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Raises: ValueError: If an invalid param is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"RaptorXSingle\" # Task-specific variables self . fasta_file = fasta_file_or_dir self . output_dir = output_dir self . param = 'RaptorX-Single/params/' + param #This is the directory where the params are stored self . extra_args = extra_args self . device = device if device == 'gpu' : self . device_id = '0' elif device == 'cpu' : self . device_id = '-1' else : self . device_id = str ( device ) self . device = 'cpu' if device == '-1' else 'gpu' # Check inputs: valid_param_list = [ 'RaptorX-Single-ESM1b.pt' , 'RaptorX-Single-ESM1v.pt' , 'RaptorX-Single-ProtTrans.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt' , 'RaptorX-Single-ESM1b-Ab.pt' , 'RaptorX-Single-ESM1v-Ab.pt' , 'RaptorX-Single-ProtTrans-Ab.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt' ] if param not in valid_param_list : raise ValueError ( f 'Invalid param: { param } . Must be one of { valid_param_list } ' ) def run ( self ): # Make the directory: self . output_dir = make_directory ( self . output_dir ) # \"python RaptorX-Single/pred.py {fasta_file} RaptorX-Single/params/{param} --plm_param_dir RaptorX-Single/params/ --out_dir {output_dir} --device {device} {extra_args}\", # Run the task self . _run_task ( self . task_name , fasta_file = self . fasta_file , param = self . param , output_dir = str ( self . output_dir ), device = self . device , device_id = self . device_id , extra_args = self . extra_args )","title":"RaptorXSingle"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.RaptorXSingle.__init__","text":"Initialize a RaptorXSingle task. Parameters: fasta_file_or_dir ( str ) \u2013 The FASTA file or directory containing multiple FASTA files. output_dir ( str , default: '.' ) \u2013 The directory to save the output files. Default is '.'. param ( str , default: 'RaptorX-Single-ESM1b.pt' ) \u2013 The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device ( str , default: 'gpu' ) \u2013 The device to run the task on. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments for the task. Default is an empty string. Raises: ValueError \u2013 If an invalid param is specified. Source code in ribbon/ribbon_tasks/ribbon_tasks/tasks.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , fasta_file_or_dir , output_dir = '.' , param = 'RaptorX-Single-ESM1b.pt' , device = 'gpu' , extra_args = \"\" ): \"\"\" Initialize a RaptorXSingle task. Args: fasta_file_or_dir (str): The FASTA file or directory containing multiple FASTA files. output_dir (str): The directory to save the output files. Default is '.'. param (str): The checkpoint to use. Default is 'RaptorX-Single-ESM1b.pt'. device (str): The device to run the task on. Default is 'gpu'. extra_args (str): Additional arguments for the task. Default is an empty string. Raises: ValueError: If an invalid param is specified. \"\"\" # Initialize the Task class super () . __init__ () # This Task name matches the name in the tasks.json file self . task_name = \"RaptorXSingle\" # Task-specific variables self . fasta_file = fasta_file_or_dir self . output_dir = output_dir self . param = 'RaptorX-Single/params/' + param #This is the directory where the params are stored self . extra_args = extra_args self . device = device if device == 'gpu' : self . device_id = '0' elif device == 'cpu' : self . device_id = '-1' else : self . device_id = str ( device ) self . device = 'cpu' if device == '-1' else 'gpu' # Check inputs: valid_param_list = [ 'RaptorX-Single-ESM1b.pt' , 'RaptorX-Single-ESM1v.pt' , 'RaptorX-Single-ProtTrans.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans.pt' , 'RaptorX-Single-ESM1b-Ab.pt' , 'RaptorX-Single-ESM1v-Ab.pt' , 'RaptorX-Single-ProtTrans-Ab.pt' , 'RaptorX-Single-ESM1b-ESM1v-ProtTrans-Ab.pt' ] if param not in valid_param_list : raise ValueError ( f 'Invalid param: { param } . Must be one of { valid_param_list } ' )","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task","text":"Source code in ribbon/runner.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Task : def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" ) def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' ) def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ] def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \"","title":"Task"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task.__init__","text":"The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Parameters: device ( str , default: 'cpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task Returns: \u2013 None Source code in ribbon/runner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , device = 'cpu' , extra_args = \"\" ): \"\"\" The Task class is the parent class for all tasks in the Ribbon framework. It contains the basic functionality for running tasks, queuing tasks, and managing task dependencies. Args: device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task Returns: None \"\"\" self . device = device self . extra_args = extra_args self . task_name = None","title":"__init__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task.__repr__","text":"Returns a string representation of the Task object. Source code in ribbon/runner.py 220 221 222 223 224 225 def __repr__ ( self ): \"\"\" Returns a string representation of the Task object. \"\"\" return f \" { self . __class__ . __name__ } \\ { self . __dict__ } \"","title":"__repr__"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task._get_task_dict","text":"Returns the dictionary for a given task. Source code in ribbon/runner.py 197 198 199 200 201 202 203 204 205 def _get_task_dict ( self , task_name ): \"\"\" Returns the dictionary for a given task. \"\"\" # Which inputs does our task require? with open ( TASKS_DIR / 'tasks.json' ) as f : tasks = json . load ( f ) return tasks [ task_name ]","title":"_get_task_dict"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task._get_task_inputs","text":"Returns the inputs required for a given task Source code in ribbon/runner.py 207 208 209 210 211 212 213 214 215 216 217 218 def _get_task_inputs ( self , task_name ): \"\"\"Returns the inputs required for a given task\"\"\" #Get the command: command = self . _get_task_dict ( task_name )[ 'command' ] #Inputs are surrounded by curly braces. Here we extract them. inputs = [ i [ 1 : - 1 ] for i in command . split () if i . startswith ( '{' ) and i . endswith ( '}' )] #Remove duplicates: inputs = list ( set ( inputs )) return inputs","title":"_get_task_inputs"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task._run_task","text":"Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Parameters: task_name ( str ) \u2013 The name of the task to run. device ( str , default: 'gpu' ) \u2013 Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args ( str , default: '' ) \u2013 Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override ( str , default: None ) \u2013 The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs ( dict , default: {} ) \u2013 Task-specific keyword arguments. Returns: \u2013 None Source code in ribbon/runner.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def _run_task ( self , task_name , scheduler = 'local' , device = 'gpu' , extra_args = \"\" , container_override = None , ** kwargs ): \"\"\" Run a task with the given name and arguments. In the child Task class, this method should be called from within the user-facing run() method. Args: task_name (str): The name of the task to run. device (str): Enables Apptainer to use GPU. Options are 'gpu', 'gpu_wsl' (if using WSL), or 'cpu'. Default is 'gpu'. extra_args (str, optional): Additional arguments to pass to the task, e.g. '--save_frequency 10 --num_steps 1000'. container_override (str, optional): The name of the container to use for the task. If not provided, the default container for that Task will be used. kwargs (dict): Task-specific keyword arguments. Returns: None \"\"\" # Add extra_args to kwargs: kwargs [ 'extra_args' ] = extra_args # Which inputs does our task require? required_inputs = self . _get_task_inputs ( task_name ) # Check that we have all the required inputs for input in required_inputs : if input not in kwargs : raise ValueError ( f 'Input { input } is required for task { task_name } ' ) # Get Information about the task: task_dict = self . _get_task_dict ( task_name ) task_name = task_dict [ 'name' ] container_name = task_dict [ 'container' ] # Allow user to override the default container (used for the Custom task): if container_override is not None : container_name = container_override print ( '--------------------------------------------' ) print ( '- Task name:' , task_name ) print ( '- Task description:' , task_dict [ 'description' ]) # Verify we have the container associated with the software we want to run. # If not, attempt to download it to the download_dir container_path = utils . verify_container ( container_name ) # Add inputs to the command, by replacing the placeholders in the command string: command = task_dict [ 'command' ] for input in required_inputs : command = command . replace ( f ' {{ { input } }} ' , str ( kwargs [ input ])) print ( '- Command:' , command ) # Set nvidia flag: nvidia_flag = { 'gpu' : '--nv' , 'gpu_wsl' : '--nvccli' , 'cpu' : '' }[ device ] # Set user-provided environment variables: env_variables_string = '' if 'environment_variables' in task_dict : if len ( task_dict [ 'environment_variables' ]) > 0 : env_variables_string = '--env ' # Join each key-value pair with a comma: env_variables_string += ',' . join ([ f ' { key } = { value } ' for key , value in task_dict [ 'environment_variables' ] . items ()]) # Run the task apptainer_command = f 'apptainer run { nvidia_flag } { env_variables_string } { container_path } { command } ' utils . run_command ( apptainer_command ) print ( '--------------------------------------------' )","title":"_run_task"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task.queue","text":"Queue the LigandMPNN task using the given scheduler. Parameters: scheduler ( str ) \u2013 The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on ( list , default: [] ) \u2013 A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type ( str , default: 'afterok' ) \u2013 The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks ( int , default: 1 ) \u2013 The number of tasks to run. Defaults to 1. time ( str , default: '1:00:00' ) \u2013 The time to allocate for the task. Defaults to '1:00:00'. mem ( str , default: '2G' ) \u2013 The memory to allocate for the task. Defaults to '2G'. auto_restart ( bool , default: True ) \u2013 Whether to automatically restart the task if it fails. Defaults to True. other_resources ( dict , default: {} ) \u2013 Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name ( str , default: None ) \u2013 The name of the job. Defaults to None. output_file ( str , default: None ) \u2013 The file to write the output to. Defaults to None. queue ( str , default: None ) \u2013 The queue to submit the task to. Defaults to None. gpus ( int , default: None ) \u2013 The number of GPUs to allocate for the task. Defaults to None. node_name ( str , default: None ) \u2013 The name of the node to run the task on. Defaults to None. Returns: str \u2013 The ID of the job in the scheduler. Source code in ribbon/runner.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def queue ( self , scheduler , depends_on = [], dependency_type = 'afterok' , n_tasks = 1 , time = '1:00:00' , mem = '2G' , auto_restart = True , other_resources = {}, job_name = None , output_file = None , queue = None , gpus = None , node_name = None ): \"\"\" Queue the LigandMPNN task using the given scheduler. Args: scheduler (str): The name of the scheduler to use. Options are 'SLURM' or 'SGE'. depends_on (list, optional): A jobID or list of jobIDs that this job depends on. (Each is an int or str). Defaults to []. dependency_type (str, optional): The type of dependency. Options are 'afterok', 'afternotok', 'afterany', 'after', 'singleton'. Defaults to 'afterok'. n_tasks (int, optional): The number of tasks to run. Defaults to 1. time (str, optional): The time to allocate for the task. Defaults to '1:00:00'. mem (str, optional): The memory to allocate for the task. Defaults to '2G'. auto_restart (bool, optional): Whether to automatically restart the task if it fails. Defaults to True. other_resources (dict, optional): Other resources to allocate for the task. Has the form {\"--option\": \"value\"}. Defaults to {}. job_name (str, optional): The name of the job. Defaults to None. output_file (str, optional): The file to write the output to. Defaults to None. queue (str, optional): The queue to submit the task to. Defaults to None. gpus (int, optional): The number of GPUs to allocate for the task. Defaults to None. node_name (str, optional): The name of the node to run the task on. Defaults to None. Returns: str: The ID of the job in the scheduler. \"\"\" # Serialize the task object to a pickle file: serialized_task = utils . serialize ( self ) # Retrieve the Ribbon container: ribbon_container_name = 'Ribbon' container_path = utils . verify_container ( ribbon_container_name ) # Retrieve the job's container: task_dict = self . _get_task_dict ( self . task_name ) job_container_name = task_dict [ 'container' ] utils . verify_container ( job_container_name ) # Correct the scheduler script mapping: batch_script_dir = Path ( MODULE_DIR ) / 'batch' / 'batch_scripts' scheduler_script = { 'SLURM' : str ( batch_script_dir / 'slurm_submit.sh' ), 'SGE' : str ( batch_script_dir / 'sge_submit.sh' )}[ scheduler ] deserialize_script = Path ( MODULE_DIR ) / 'deserialize_and_run.py' # Prepare job variables: job_variables = f \"ribbon_container= { container_path } ,\" \\ f \"ribbon_deserialize_script= { deserialize_script } ,\" \\ f \"serialized_job= { serialized_task } ,\" \\ f \"RIBBON_TASKS_DIR= { os . getenv ( 'RIBBON_TASKS_DIR' ) } ,\" \\ f \"DEVICE= { self . device } \" ###################################### # Prepare the resources: # TODO: this is messy, we should clean this up later resources = { 'time' : time , 'mem' : mem } if depends_on : resources [ 'dependency' ] = depends_on if gpus : resources [ 'gpus' ] = gpus if job_name : resources [ 'job-name' ] = job_name if auto_restart : resources [ 'requeue' ] = True # Use True to indicate a flag without a value if output_file : resources [ 'output' ] = output_file if queue : resources [ 'queue' ] = queue if node_name : resources [ 'node-name' ] = node_name # Note: We don't parse other_resouces in the same way - we just pass them through as-is, # assuming the user has formatted them correctly. ######################################################### # Generate the command using queue_utils if scheduler == 'SLURM' : command = queue_utils . generate_slurm_command ( resources , other_resources , job_variables , scheduler_script ) elif scheduler == 'SGE' : command = queue_utils . generate_sge_command ( resources , other_resources , job_variables , scheduler_script ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) # Run the task: stdout , stderr = utils . run_command ( command , capture_output = True ) print ( stdout , stderr ) # Parse the job ID from the output: if scheduler == 'SLURM' : job_id = queue_utils . parse_slurm_output ( stdout ) elif scheduler == 'SGE' : job_id = queue_utils . parse_sge_output ( stdout ) else : raise ValueError ( f \"Unsupported scheduler: { scheduler } \" ) return job_id","title":"queue"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.Task.run","text":"Run the task. This method should be overridden by the child class. Source code in ribbon/runner.py 25 26 27 28 29 def run ( self ): \"\"\" Run the task. This method should be overridden by the child class. \"\"\" raise NotImplementedError ( f \"You are attempting to run a task { self . __class__ . __name__ } without defining a run method.\" )","title":"run"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.list_files","text":"Returns a list of files in a directory with a given extension Source code in ribbon/utils.py 15 16 17 def list_files ( directory , extension ): \"\"\"Returns a list of files in a directory with a given extension\"\"\" return [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if f . endswith ( extension )]","title":"list_files"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.make_directories","text":"Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. Source code in ribbon/utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_directories ( * directories ): \"\"\" Creates directories if they do not exist. Returns a list of Path objects, in case they were strings. \"\"\" new_directories = [] for directory in directories : # Check it's a Path object: if not isinstance ( directory , Path ): directory = Path ( directory ) directory . mkdir ( parents = True , exist_ok = True ) new_directories . append ( directory ) return new_directories","title":"make_directories"},{"location":"api/ribbon_tasks/ribbon_tasks/#ribbon.ribbon_tasks.ribbon_tasks.make_directory","text":"Creates a directory if it does not exist. Parameters: directory ( str or Path ) \u2013 The directory to create. Returns: Path \u2013 path object of the created directory. Source code in ribbon/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def make_directory ( directory ): \"\"\" Creates a directory if it does not exist. Args: directory (str or Path): The directory to create. Returns: Path: path object of the created directory. \"\"\" directory = make_directories ( directory )[ 0 ] return directory","title":"make_directory"},{"location":"usage/1_protein_folding/","text":"A Simple Example: Protein Folding For this series of tutorials, all the code will be in the examples directory in the repo . After installing , Apptainer and ribbon, use the code in /examples/example1 to follow along! Let's say we want to fold a protein from an amino acid sequence. We'll use Chai-1 , a high-accuracy, ligand aware protein folding method. This requires a gpu! First, we create a Task object, which sets up our Chai-1 job: import ribbon my_folding_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) Then, we run the Task : my_folding_task.run() This can be shortened to a single command: import ribbon ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ).run() Why the long wait? When you run a Task for the first time, Ribbon will download all of the relevant software and scripts to your computer inside of a virtual machine. This only happens once !","title":"A Simple Example: Protein Folding"},{"location":"usage/1_protein_folding/#a-simple-example-protein-folding","text":"For this series of tutorials, all the code will be in the examples directory in the repo . After installing , Apptainer and ribbon, use the code in /examples/example1 to follow along! Let's say we want to fold a protein from an amino acid sequence. We'll use Chai-1 , a high-accuracy, ligand aware protein folding method. This requires a gpu! First, we create a Task object, which sets up our Chai-1 job: import ribbon my_folding_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) Then, we run the Task : my_folding_task.run() This can be shortened to a single command: import ribbon ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ).run() Why the long wait? When you run a Task for the first time, Ribbon will download all of the relevant software and scripts to your computer inside of a virtual machine. This only happens once !","title":"A Simple Example: Protein Folding"},{"location":"usage/2_pipelining/","text":"Let's Get Pipelining Since running a job is so simple, we can string many jobs together into a pipeline . (Code: /examples/example2 ) For example, we'll start with an initial PDB structure. We'll redesign the sequence using LigandMPNN, which will output several FASTA files that it thinks could fold into our input structure: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ).run() I wonder how well LigandMPNN did? We can easily fold these sequences using the fast RaptorX-Single software: ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx' ).run() Chaining these together, we get our first protein design pipeline: import ribbon # First, we create 5 new sequences for this structure: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ).run() # These sequences are split into individual files, and are stored in 'out/seqs_split' # Then, we fold using RaptorX: ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx' ).run() What are these files? Certain Tasks need to create temporary files in the current directory while running (Such as a the LigandMPNN/ folder, or RaptorX-Single symlink). In the future, I'd like Ribbon to clean these up better. For now, these files are safe to delete as long as no Tasks are running .","title":"Let's Get Pipelining"},{"location":"usage/2_pipelining/#lets-get-pipelining","text":"Since running a job is so simple, we can string many jobs together into a pipeline . (Code: /examples/example2 ) For example, we'll start with an initial PDB structure. We'll redesign the sequence using LigandMPNN, which will output several FASTA files that it thinks could fold into our input structure: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ).run() I wonder how well LigandMPNN did? We can easily fold these sequences using the fast RaptorX-Single software: ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx' ).run() Chaining these together, we get our first protein design pipeline: import ribbon # First, we create 5 new sequences for this structure: ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ).run() # These sequences are split into individual files, and are stored in 'out/seqs_split' # Then, we fold using RaptorX: ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx' ).run() What are these files? Certain Tasks need to create temporary files in the current directory while running (Such as a the LigandMPNN/ folder, or RaptorX-Single symlink). In the future, I'd like Ribbon to clean these up better. For now, these files are safe to delete as long as no Tasks are running .","title":"Let's Get Pipelining"},{"location":"usage/3_queueing_part_1/","text":"Queueing Pt. 1: Submitting Jobs If we want to run hundreds or thousands of Tasks , it's impractical to run them one-by-one. We can use a scheduler like SLURM or SGE to queue Tasks (called jobs, in this context). A high-performance compute cluster will use one of these tools to manage jobs. (Code: /examples/example3 ) In this tutorial, we will not cover how to use SGE or SLURM. We recommend becoming familiar with these tools on your compute cluster before continuing. To start, let's make a new Task object to fold a protein with Chai-1: import ribbon my_folding_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) Before, we ran the task locally using my_folding_task.run() . To queue the job, we instead use the .queue() function: my_folding_task.queue( scheduler='SLURM', ) In this case, I've specified that the scheduler our cluster is using is SLURM. Let's instead submit a job to an SGE cluster. We can also specify add a job name and a named output file. Additionally, let's make sure we're running on the GPU queue for our cluster, and ask only for nodes that have an A40 GPU. Finally, we'll have our job quit early if it runs longer than 30 minutes. my_folding_task.queue( scheduler='SGE', queue='gpu.q', job_name='my_chai1_job', output_file='my_chai1_job.out', node_name = 'qb3-atgpu*', time='00:30:00') Putting it all together: import ribbon my_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) my_task.queue(scheduler='SGE', job_name='my_chai1_job', output_file='my_chai1_job.out', time='00:30:00', queue='gpu.q') RTFM These specific parameters (including queue and node names) will vary for your compute cluster. Make sure to read your cluster's documentation, and test using the SLURM or SGE command line interface.","title":"Queueing Pt. 1: Submitting Jobs"},{"location":"usage/3_queueing_part_1/#queueing-pt-1-submitting-jobs","text":"If we want to run hundreds or thousands of Tasks , it's impractical to run them one-by-one. We can use a scheduler like SLURM or SGE to queue Tasks (called jobs, in this context). A high-performance compute cluster will use one of these tools to manage jobs. (Code: /examples/example3 ) In this tutorial, we will not cover how to use SGE or SLURM. We recommend becoming familiar with these tools on your compute cluster before continuing. To start, let's make a new Task object to fold a protein with Chai-1: import ribbon my_folding_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) Before, we ran the task locally using my_folding_task.run() . To queue the job, we instead use the .queue() function: my_folding_task.queue( scheduler='SLURM', ) In this case, I've specified that the scheduler our cluster is using is SLURM. Let's instead submit a job to an SGE cluster. We can also specify add a job name and a named output file. Additionally, let's make sure we're running on the GPU queue for our cluster, and ask only for nodes that have an A40 GPU. Finally, we'll have our job quit early if it runs longer than 30 minutes. my_folding_task.queue( scheduler='SGE', queue='gpu.q', job_name='my_chai1_job', output_file='my_chai1_job.out', node_name = 'qb3-atgpu*', time='00:30:00') Putting it all together: import ribbon my_task = ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ) my_task.queue(scheduler='SGE', job_name='my_chai1_job', output_file='my_chai1_job.out', time='00:30:00', queue='gpu.q') RTFM These specific parameters (including queue and node names) will vary for your compute cluster. Make sure to read your cluster's documentation, and test using the SLURM or SGE command line interface.","title":"Queueing Pt. 1: Submitting Jobs"},{"location":"usage/3_queueing_part_2/","text":"Queueing Pt. 2: Linking Jobs Often, we'll want to run Tasks one after the other, where each depends on the previous. Like in (Let's get Pipelining), this allows us to form complex protein design pipelines. In this example, we'll schedule a job to generate protein sequences using LigandMPNN, and schedule a subsequent job to fold the sequences using RaptorX-Single. (Code: /examples/example4 ) We'll start as before, defining a LigandMPNN Task : import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) Then, we'll queue this task. The queue() function returns the job id , which is a unique identifier for the job we've submitted to the scheduler. lmpnn_job_id = lmpnn_task.queue(scheduler='SGE') We create and submit a RaptorX-Single Task , using the depends_on parameter to pass in a list of job IDs. raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SGE' depends_on = [lmpnn_job_id] ) Dependency Types By default, the job will only run after all dependencies complete successfully . If using SLURM, more complex behavior can be set using the dependency_type parameter, which takes standard SLURM dependency types (default: afterok ). Here's our script completed: import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) # We'll queue the job, and get the job ID lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM', depends_on = [lmpnn_job_id] )","title":"Queueing Pt. 2: Linking Jobs"},{"location":"usage/3_queueing_part_2/#queueing-pt-2-linking-jobs","text":"Often, we'll want to run Tasks one after the other, where each depends on the previous. Like in (Let's get Pipelining), this allows us to form complex protein design pipelines. In this example, we'll schedule a job to generate protein sequences using LigandMPNN, and schedule a subsequent job to fold the sequences using RaptorX-Single. (Code: /examples/example4 ) We'll start as before, defining a LigandMPNN Task : import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) Then, we'll queue this task. The queue() function returns the job id , which is a unique identifier for the job we've submitted to the scheduler. lmpnn_job_id = lmpnn_task.queue(scheduler='SGE') We create and submit a RaptorX-Single Task , using the depends_on parameter to pass in a list of job IDs. raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SGE' depends_on = [lmpnn_job_id] ) Dependency Types By default, the job will only run after all dependencies complete successfully . If using SLURM, more complex behavior can be set using the dependency_type parameter, which takes standard SLURM dependency types (default: afterok ). Here's our script completed: import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) # We'll queue the job, and get the job ID lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM', depends_on = [lmpnn_job_id] )","title":"Queueing Pt. 2: Linking Jobs"},{"location":"usage/3_queueing_part_3/","text":"Queueing Pt. 3: Mixed Submission As seen in the previous tutorial, we can link together complex sets of jobs using the depends_on flag. In this way, we can submit many interdependent jobs ahead of time. (Code: /examples/example4 ) However, it's sometimes useful to run small snippets of python code between jobs, or to have programmatic checks to make sure everything's running smoothly (before we submit thousands of jobs that inevitably fail due to a typo). In these instances, we can use a mixed submission strategy. Rather than submitting all of our Tasks to a scheduler at the start, we can queue a set of jobs, wait for them to finish, and check on or modify the results before submitting further jobs. In this example, we'll predict sequences using LigandMPNN, but before we fold them with RaptorX-Single, we'll apply mutations to those sequences using a simple python function. We start by queueing our first job: import ribbon lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') Then we'll use ribbon.wait_for_jobs() , to pause our script until our jobs have completed successfully: ribbon.wait_for_jobs([lmpnn_job_id], queue='SLURM') This simple function periodically queries the scheduler, keeping track of the status of our jobs. After all of our jobs have completed, we apply a small mutation to each using the following script: position = 10 mutation = 'A' import os os.mkdir('./out/lmpnn/seqs_split_mutated') for f in os.listdir('./out/lmpnn/seqs_split'): with open(os.path.join('./out/lmpnn/seqs_split', f)) as infile, open(os.path.join('./out/lmpnn/seqs_split_mutated', f), 'w') as outfile: for line in infile: outfile.write(line if line.startswith('>') else line[:position-1] + mutation + line[position:]) You'll likely want at more sophisticated method for mutagenesis - but for a tutorial, this will suffice! Now, we can fold these mutated structures, as before: # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn/seqs_split', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM', depends_on = [lmpnn_job_id] ) All together, our final script looks like: import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) # We'll queue the job, and get the job ID lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') # Wait for it to finish: ribbon.wait_for_jobs([lmpnn_job_id], scheduler='SLURM') # For all of our output FASTAs, apply a mutation: position = 10 mutation = 'A' import os os.mkdir('./out/lmpnn/seqs_split_mutated') for f in os.listdir('./out/lmpnn/seqs_split'): with open(os.path.join('./out/lmpnn/seqs_split', f)) as infile, open(os.path.join('./out/lmpnn/seqs_split_mutated', f), 'w') as outfile: for line in infile: outfile.write(line if line.startswith('>') else line[:position-1] + mutation + line[position:]) # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn/seqs_split', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM' ) If we execute this script manually, it will run until the final jobs are queued. However, if this script exits prematurely - for example, if your ssh connection closes, your final jobs may not be queued. To overcome this limitation, we can submit this script as the primary SLURM job, which queues the LigandMPNN and RaptorX-Single Tasks as secondary jobs. After saving our script, we can create a minimal SLURM submission script called submit.sh : #!/bin/bash conda activate ribbon python mixed_scheduling.py And submit our primary job with the SLURM command-line interface: sbatch submit.sh","title":"Queueing Pt. 3: Mixed Submission"},{"location":"usage/3_queueing_part_3/#queueing-pt-3-mixed-submission","text":"As seen in the previous tutorial, we can link together complex sets of jobs using the depends_on flag. In this way, we can submit many interdependent jobs ahead of time. (Code: /examples/example4 ) However, it's sometimes useful to run small snippets of python code between jobs, or to have programmatic checks to make sure everything's running smoothly (before we submit thousands of jobs that inevitably fail due to a typo). In these instances, we can use a mixed submission strategy. Rather than submitting all of our Tasks to a scheduler at the start, we can queue a set of jobs, wait for them to finish, and check on or modify the results before submitting further jobs. In this example, we'll predict sequences using LigandMPNN, but before we fold them with RaptorX-Single, we'll apply mutations to those sequences using a simple python function. We start by queueing our first job: import ribbon lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') Then we'll use ribbon.wait_for_jobs() , to pause our script until our jobs have completed successfully: ribbon.wait_for_jobs([lmpnn_job_id], queue='SLURM') This simple function periodically queries the scheduler, keeping track of the status of our jobs. After all of our jobs have completed, we apply a small mutation to each using the following script: position = 10 mutation = 'A' import os os.mkdir('./out/lmpnn/seqs_split_mutated') for f in os.listdir('./out/lmpnn/seqs_split'): with open(os.path.join('./out/lmpnn/seqs_split', f)) as infile, open(os.path.join('./out/lmpnn/seqs_split_mutated', f), 'w') as outfile: for line in infile: outfile.write(line if line.startswith('>') else line[:position-1] + mutation + line[position:]) You'll likely want at more sophisticated method for mutagenesis - but for a tutorial, this will suffice! Now, we can fold these mutated structures, as before: # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn/seqs_split', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM', depends_on = [lmpnn_job_id] ) All together, our final script looks like: import ribbon # First, we create 5 new sequences for this structure: lmpnn_task = ribbon.LigandMPNN( structure_list = ['my_structure.pdb'], output_dir = './out/lmpnn', num_designs = 5 ) # We'll queue the job, and get the job ID lmpnn_job_id = lmpnn_task.queue(scheduler='SLURM') # Wait for it to finish: ribbon.wait_for_jobs([lmpnn_job_id], scheduler='SLURM') # For all of our output FASTAs, apply a mutation: position = 10 mutation = 'A' import os os.mkdir('./out/lmpnn/seqs_split_mutated') for f in os.listdir('./out/lmpnn/seqs_split'): with open(os.path.join('./out/lmpnn/seqs_split', f)) as infile, open(os.path.join('./out/lmpnn/seqs_split_mutated', f), 'w') as outfile: for line in infile: outfile.write(line if line.startswith('>') else line[:position-1] + mutation + line[position:]) # Then, we create and queue a RaptorX Task: raptorx_task = ribbon.RaptorXSingle( fasta_file_or_dir = './out/lmpnn/seqs_split', output_dir = './out/raptorx', ) raptorx_task.queue( scheduler='SLURM' ) If we execute this script manually, it will run until the final jobs are queued. However, if this script exits prematurely - for example, if your ssh connection closes, your final jobs may not be queued. To overcome this limitation, we can submit this script as the primary SLURM job, which queues the LigandMPNN and RaptorX-Single Tasks as secondary jobs. After saving our script, we can create a minimal SLURM submission script called submit.sh : #!/bin/bash conda activate ribbon python mixed_scheduling.py And submit our primary job with the SLURM command-line interface: sbatch submit.sh","title":"Queueing Pt. 3: Mixed Submission"},{"location":"usage/4_custom_tasks/","text":"","title":"4 custom tasks"},{"location":"usage/getting_started/","text":"How to use Ribbon Ribbon is an easy-to-use python package, which simplifies the installation and running of protein design software. Want to fold a protein? Once Ribbon is installed , it's as easy as: import ribbon ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ).run() To get started, move on to our initial tutorial: A Simple Example: Protein Folding","title":"How to use Ribbon"},{"location":"usage/getting_started/#how-to-use-ribbon","text":"Ribbon is an easy-to-use python package, which simplifies the installation and running of protein design software. Want to fold a protein? Once Ribbon is installed , it's as easy as: import ribbon ribbon.Chai1( fasta_file = 'my_sequence.fasta', # Input FASTA output_dir = './out' # Where the outputs will be stored ).run() To get started, move on to our initial tutorial: A Simple Example: Protein Folding","title":"How to use Ribbon"}]}